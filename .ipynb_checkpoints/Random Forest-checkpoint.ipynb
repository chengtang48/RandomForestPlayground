{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest class from scratch\n",
    "## My goals are\n",
    "- Explore RF with tranditional decision trees (kd-trees)\n",
    "- Explore RF approach with RP-trees\n",
    "- Explore RF with oblique splits (PCA, LDA, etc)\n",
    "- Explore RF with preprocessings\n",
    " * Random rotation (Vempala)\n",
    " * Randomer forest\n",
    " * Output space random projections\n",
    "\n",
    "Building of random forest (bagging technique) is adapted from tutorial: http://machinelearningmastery.com/implement-random-forest-scratch-python/\n",
    "and part of the design of my spatial-tree class is inspired by \"spatialtree\" package:https://github.com/bmcfee/spatialtree\n",
    "(note: I believe my design is better than that of Brian McFee, since it is more concise and flexible)\n",
    "\n",
    "On sources of data compression:\n",
    " - Forest has three sources of data compression, which may or may not involve randomness: \n",
    "  * preprocessing such as PCA, random projection, random rotation, etc, which can be applied either for a single tree or for the entire forest and achieves\n",
    "    - feature selection/extraction for the entire forest\n",
    "      * This includes the theoretically justified randomly oriented kd-tree in [Vempala 12]\n",
    "    - feature selection/extraction for a single tree\n",
    "  * data subsampling for each tree, which adds randomness\n",
    "  * feature selection/extraction at each tree node (before splitting direction and value are determined)\n",
    "\n",
    " - A generalized forest where methods to achieve compression vary at the node level in [Tomita, Maggioni, Vogelstein 16], where at each step a p by d projection matrix A is chosen; this subsumes\n",
    "  * Forest-IC, Forest-RC in Breiman: sparsity constrained projection matrix sampling\n",
    "  * Rotation forest: deterministic projection matrix via PCA\n",
    "  * Randomer forest: sparse projection matrix via \"very sparse random projections\" \n",
    "  * RP-tree proposed in Dasgupta can also be viewed as a special case here, where the matrix A is a 1-D dense or sparse projection vector\n",
    "(note: when d>1, after projection, the splitting rule must be able to pick a coordinate for splitting, so the first three approaches are more suitable for supervised tasks where coordinate-selection can be made via labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from six.moves import cPickle as pickle\n",
    "import random \n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "from math import floor\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import random_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###---------- Utility functions\n",
    "## computing spread of one-dim data\n",
    "def spread_1D(data_1D):\n",
    "    return np.max(data_1D)-np.min(data_1D)\n",
    "\n",
    "## computing data diamter of a cell\n",
    "def data_diameter(data):\n",
    "    \"\"\"\n",
    "    Input data is assumed to be confined in the\n",
    "    desired cell\n",
    "    \"\"\"\n",
    "    dist, indx, indy = 0, None, None\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(i+1, data.shape[0]):\n",
    "            dist_new = np.linalg.norm(data[i,:]-data[j,:])\n",
    "            if dist_new > dist:\n",
    "                dist, indx, indy = dist_new, i, j\n",
    "    return dist, indx, indy\n",
    "\n",
    "###---------- compressive projection matrix designs\n",
    "def comp_projmat(data, **kwargs):\n",
    "    \"\"\"\n",
    "    returns a projection matrix\n",
    "    \"\"\"\n",
    "    namelist = ['breiman','ho','tomita', 'dasgupta']\n",
    "    assert kwargs['name'] in namelist, \"No such method for constructing projection matrix!\"\n",
    "    \n",
    "    if kwargs['name'] == 'breiman':\n",
    "        ## Breiman's Forest-IC and Forest-RC\n",
    "        s = kwargs['sparsity']\n",
    "        d = kwargs['target_dim']\n",
    "        A = np.zeros((data.shape[1],d))\n",
    "        ## sample sparsity-constrained A\n",
    "        for i in range(d):\n",
    "            ind = np.random.choice(data.shape[1], size=s, replace=False)\n",
    "            if s == 1:\n",
    "                A[ind,i] = 1\n",
    "            else:\n",
    "                for j in range(len(ind)):\n",
    "                    A[ind[j], i] = np.random.uniform(-1,1)\n",
    "    \n",
    "    elif kwargs['name'] == 'ho':\n",
    "        ## rotation forest\n",
    "        d = kwargs['target_dim']\n",
    "        ## find A by PCA\n",
    "    elif kwargs['name'] == 'tomita':\n",
    "        ## randomer forest\n",
    "        d = kwargs['target_dim']\n",
    "        ## sample sparse A via very sparse rp\n",
    "        density = 1/(data.shape[1]**(1/2)) #default density value\n",
    "        if 'density' in kwargs:\n",
    "            if kwargs['density'] <= 1 and kwargs['density']>0:\n",
    "                density = kwargs['density']\n",
    "        \n",
    "        transformer = random_projection.SparseRandomProjection(n_components=d, density=density)    \n",
    "        transformer.fit(data)\n",
    "        A = transformer.components_.copy()\n",
    "        A = A.T\n",
    "        \n",
    "    else:\n",
    "        ## dasgupta rp-tree \n",
    "        d = 1 # default to a random vector          \n",
    "        if 'target_dim' in kwargs:\n",
    "            d = kwargs['target_dim']\n",
    "        n_features = data.shape[1]\n",
    "        A = np.zeros((data.shape[1], d))\n",
    "        # sample dense projection matrix\n",
    "        for i in range(d):\n",
    "            A[:,i] = np.random.normal(0, 1/np.sqrt(n_features), n_features)\n",
    "\n",
    "    return A\n",
    "\n",
    "#######-------split designs\n",
    "\n",
    "## cart splits\n",
    "def cart_split(data, proj_mat, labels=None, regress=False):\n",
    "    # test for the best split feature and threshold on data CART criterion\n",
    "    data_trans = np.dot(data, proj_mat) # n-by-d\n",
    "    score, ind, thres = -999, None, None\n",
    "    for i in range(data_trans.shape[1]):\n",
    "        if not regress:\n",
    "            # classification\n",
    "            score_new, thres_new = cscore(data_trans[:,i], labels)\n",
    "        else:\n",
    "            # regression\n",
    "            score_new, thres_new = rscore(data_trans[:,i], labels)\n",
    "        if score_new > score:\n",
    "            score = score_new\n",
    "            ind = i\n",
    "            thres = thres_new\n",
    "    w = proj_mat[:,ind]\n",
    "    return score, w, thres\n",
    "\n",
    "def cscore(data_1D, labels):\n",
    "    ## cart classification criterion\n",
    "    n = len(labels)\n",
    "    p1 = np.mean(labels)\n",
    "    data_sorted, ind_sorted = np.sort(data_1D), np.argsort(data_1D)\n",
    "    score, thres = -999, None\n",
    "    for i in range(1,n):\n",
    "        cell_l = ind_sorted[:i]\n",
    "        cell_r = ind_sorted[i:]\n",
    "        split_val = data_sorted[i]\n",
    "        p1_l = np.mean(labels[cell_l])\n",
    "        p1_r = np.mean(labels[cell_r])\n",
    "        n_l = len(cell_l)\n",
    "        score_new = p1*(1-p1) - (n_l/n)*(1-p1_l)*p1_l - (n-n_l)/n*(1-p1_r)*p1_r  \n",
    "        if score_new > score:\n",
    "            score = score_new\n",
    "            thres = split_val\n",
    "    return score, thres\n",
    "\n",
    "def rscore(data_1D, labels):\n",
    "    ## cart regression criterion\n",
    "    n = len(labels)\n",
    "    ybar = np.mean(labels)\n",
    "    data_sorted, ind_sorted = np.sort(data_1D), np.argsort(data_1D)\n",
    "    score, thres = -999, None\n",
    "    for i in range(1,n):\n",
    "        cell_l = ind_sorted[:i]\n",
    "        cell_r = ind_sorted[i:]\n",
    "        split_val = data_sorted[i]\n",
    "        ybar_l = np.mean(labels[cell_l])\n",
    "        ybar_r = np.mean(labels[cell_r])\n",
    "        score_new =(np.sum((labels-ybar)**2)-np.sum((labels[cell_l]-ybar_l)**2)\\\n",
    "         -np.sum((labels[cell_r]-ybar_r)**2))/n\n",
    "        if score_new > score:\n",
    "            score = score_new\n",
    "            thres = split_val\n",
    "    return score, thres\n",
    "\n",
    "##### median splits\n",
    "\n",
    "def median_split(data, proj_mat, labels=None):\n",
    "    data_transformed = np.dot(data, proj_mat)\n",
    "    if proj_mat.ndim > 1:\n",
    "        score, ind = 0, 0\n",
    "        for i in range(proj_mat.shape[1]):\n",
    "            score_new = spread_1D(data_transformed[i])\n",
    "            if score_new > score:\n",
    "                score, ind = scorn_new, i\n",
    "        w = proj_mat[:, ind]\n",
    "        thres = np.median(data_transformed[:,ind])\n",
    "    else:\n",
    "        thres = np.median(data_transformed)\n",
    "        w = proj_mat\n",
    "        score = spread_1D(data_transformed)\n",
    "    return score, w, thres\n",
    "\n",
    "def median_perturb_split(data, proj_mat, node_height, labels=None, diameter=None, root_height=None):\n",
    "    assert root_height is not None, \"Please provide the level of the root!\"\n",
    "    if (node_height+root_height) % 2 == 0:\n",
    "        # normal median splits\n",
    "        return median_split(data, proj_mat, labels=labels)\n",
    "    else:\n",
    "        assert diameter is not None, \"Diameter of the cell must be given!\"\n",
    "        # noisy splits\n",
    "        data_transformed = np.dot(data, proj_mat)\n",
    "        if proj_mat.ndim > 1:\n",
    "            score, ind = 0, 0\n",
    "            for i in range(proj_mat.shape[1]):\n",
    "                score_new = spread_1D(data_transformed[i])\n",
    "                if score_new > score:\n",
    "                    score, ind = score_new, i\n",
    "            w = proj_mat[:, ind]\n",
    "            jitter = np.random.uniform(-1,1) * 6/np.sqrt(data.shape[1]) * diameter\n",
    "            thres = np.median(data_transformed[:,ind])+jitter\n",
    "        \n",
    "        else:\n",
    "            jitter = np.random.uniform(-1,1) * 6/np.sqrt(data.shape[1]) * diameter\n",
    "            thres = np.median(data_transformed)+jitter\n",
    "            w = proj_mat\n",
    "            score = spread_1D(data_transformed)\n",
    "        \n",
    "    \n",
    "        return score, w, thres\n",
    "\n",
    "## 2-means split\n",
    "def two_means_split(data, proj_mat, labels=None):\n",
    "    # this essentially defines a hierarchical clustering on data\n",
    "    return score, w, thres\n",
    "\n",
    "#######-------stop rules\n",
    "def naive_stop_rule(data, height=None):\n",
    "    if data.shape[0] < 4:\n",
    "        return True\n",
    "    if height > 10:\n",
    "        ## DO NOT ever make it exceed 15!!!\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def cell_size_rule(data, target_diameter=None):\n",
    "    ddiameter,_,_ = data_diameter(data)\n",
    "    \n",
    "    if ddiameter <= target_diameter:\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## A class of binary spatial-trees \n",
    "        \n",
    "class flex_binary_trees(object):\n",
    "    \"\"\"\n",
    "    A recursive data structure based on binary trees\n",
    "     - at each node, it contains data, left, right child (or none if leaf), just as any other tree\n",
    "     - it also knows its height\n",
    "     - additionally, it has information about split direction and split threshold\n",
    "     \n",
    "    On splitting method\n",
    "     - if rpart or cpart are used, labels must be provided\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, data_indices=None, \n",
    "                 proj_design={'name':'projmat','params':{'name':'breiman','sparsity':3,'target_dim':10}}, \n",
    "                 split_design={'name':'cart', 'params':{'regress':False}}, \n",
    "                 stop_design={'name':'naive'}, \n",
    "                 height=0, labels=None, master_tree=None):\n",
    "        \"\"\"\n",
    "        data: n by d matrix, the entire dataset assigned to the tree\n",
    "        data_indices: the subset of indices assigned to this node\n",
    "        proj_design: A dictionary that contains name and params of a method; \n",
    "          the method returns one or more splitting directions (projection matrix)\n",
    "        split_design: A dict that contains name and params of a function;\n",
    "          the function s.t. given 1D projected data, it must return the splitting threshold\n",
    "        stop_rule: a boolean function of data_indices and height\n",
    "        height: height of current node (root has 0 height)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.data_ind = data_indices\n",
    "        if data_indices is None:\n",
    "            self.data_ind = np.ones(data.shape[0], dtype=bool)\n",
    "        self.proj_design = proj_design\n",
    "        self.split_design = split_design \n",
    "        self.stop_design = stop_design\n",
    "        \n",
    "        self.height_ = height\n",
    "        self.labels = labels\n",
    "        self.master_tree = master_tree\n",
    "        self.leftChild_ = None\n",
    "        self.rightChild_ = None\n",
    "            \n",
    "            \n",
    "        ## set stop rule: a boolean function\n",
    "        if self.stop_design['name'] == 'naive':\n",
    "            self.isLeaf = naive_stop_rule(self.data[self.data_ind,:], height=self.height_) \n",
    "            \n",
    "        elif self.stop_design['name'] == 'cell_size':\n",
    "            self.isLeaf = cell_size_rule(self.data[self.data_ind,:], \n",
    "                                         target_diameter=self.stop_design['diameter']/2)\n",
    "            \n",
    "    \n",
    "    def proj_rule_function(self):\n",
    "        \"\"\"\n",
    "        A function such that given name of a method, returns a projection vector (splitting direction)\n",
    "        Can be override (user-defined)\n",
    "        returns a n_features by n_projected_dim projection matrix, A\n",
    "        \"\"\"\n",
    "        name_list = ['projmat', 'cyclic', 'full']\n",
    "        \n",
    "        method = self.proj_design['name']\n",
    "        \n",
    "        assert method in name_list, 'No such rule implemented in the current tree class!'\n",
    "        \n",
    "        if method == 'projmat':\n",
    "            \n",
    "            return comp_projmat(self.data[self.data_ind,:], **self.proj_design['params'])\n",
    "        \n",
    "        elif method == 'cyclic':\n",
    "            # cycle through features using height information\n",
    "            # here A is 1D\n",
    "            n_features = self.data.shape[1]\n",
    "            A = np.zeros(n_features)\n",
    "            A[self.height_ % n_features] = 1\n",
    "            return A\n",
    "        else:\n",
    "            # no compression, 'full'\n",
    "            return np.eye(self.data.shape[1])\n",
    "        \n",
    "    def split_rule_function(self, A):\n",
    "        name_list = ['cart', 'median', 'median_perturb', '2means']\n",
    "        \n",
    "        method = self.split_design['name']\n",
    "        assert method in name_list, 'No such split rule implemented in current tree class!'\n",
    "        \n",
    "        params = self.split_design['params']\n",
    "        \n",
    "        if method == 'cart':\n",
    "            return cart_split(self.data[self.data_ind,:], A, self.labels[self.data_ind], **params)\n",
    "        elif method == 'median':\n",
    "            return median_split(self.data[self.data_ind,:], A, **params)\n",
    "        elif method == 'median_perturb':\n",
    "            node_height = self.height_\n",
    "            return median_perturb_split(self.data[self.data_ind,:], A, node_height, **params)\n",
    "        else:\n",
    "            return two_means_split(self.data[self.data_ind,:], A, **params)\n",
    "        \n",
    "    \n",
    "    def buildtree(self):\n",
    "        \"\"\"\n",
    "        Recursively build a tree starting from current node as root\n",
    "        Constructs w (projection direction) and threshold for each node\n",
    "        \"\"\"\n",
    "        if not self.isLeaf:\n",
    "            ## set projection/transformation/selection matrix\n",
    "            A = self.proj_rule_function() \n",
    "            ## transform data to get one or more candidate features \n",
    "            #projected_data = np.dot(self.data[self.data_ind,:], self.w_)\n",
    "    \n",
    "            ## find the best split feature and the best threshold\n",
    "            #_, self.thres_ = self.split_rule(projected_data) ###### change the input to *args in the future\n",
    "            split_rule = self.split_design['name']\n",
    "            _, self.w_, self.thres_ = self.split_rule_function(A)\n",
    "            \n",
    "            projected_data = np.dot(self.data[self.data_ind, :], self.w_) ## project data to 1-D\n",
    "            \n",
    "            data_indices = []\n",
    "            for i in range(len(self.data_ind)):\n",
    "                if self.data_ind[i] == 1:\n",
    "                    data_indices.append(i)\n",
    "            assert len(data_indices) == len(projected_data)\n",
    "            data_indices = np.array(data_indices)\n",
    "            \n",
    "            ## split data into left and right\n",
    "            left_indices = projected_data < self.thres_\n",
    "            right_indices = projected_data >= self.thres_\n",
    "            \n",
    "            assert np.sum(left_indices)+np.sum(right_indices) == len(data_indices)\n",
    "            \n",
    "            left_ind = data_indices[left_indices]\n",
    "            right_ind = data_indices[right_indices]\n",
    "            ##\n",
    "            n_data = self.data.shape[0]\n",
    "            left = np.zeros(n_data, dtype=bool)\n",
    "            left[left_ind] = 1\n",
    "            right = np.zeros(n_data, dtype=bool)\n",
    "            right[right_ind] = 1\n",
    "            ## build subtrees on left and right partitions\n",
    "            self.leftChild_ = flex_binary_trees(self.data, left, self.proj_design, self.split_design, self.stop_design, self.height_+1, self.labels)\n",
    "            self.leftChild_.buildtree()\n",
    "            self.rightChild_ = flex_binary_trees(self.data, right, self.proj_design, self.split_design, self.stop_design, self.height_+1, self.labels)\n",
    "            self.rightChild_.buildtree()\n",
    "            \n",
    "        \n",
    "    def train(self):\n",
    "        self.buildtree()\n",
    "        \n",
    "    def predict_one(self, point, predict_type='class'):\n",
    "        return predict_one_bt(self, point, predict_type=predict_type)\n",
    "        \n",
    "    def predict(self, test, predict_type='class'):\n",
    "        return predict_bt(self, test, predict_type=predict_type)\n",
    "            \n",
    "            \n",
    "####-------- Utility functions for binary trees   \n",
    "\n",
    "def retrievalLeaf(btree, query):\n",
    "    \"\"\"\n",
    "    Given a binary partition tree\n",
    "    returns leaf node that contains query\n",
    "    \"\"\"\n",
    "    if btree.leftChild_ is None and btree.rightChild_ is None:\n",
    "        return btree\n",
    "    if np.dot(btree.w_, query) < btree.thres_:\n",
    "        return retrievalLeaf(btree.leftChild_, query)\n",
    "    else:\n",
    "        return retrievalLeaf(btree.rightChild_, query)\n",
    "    \n",
    "def retrievalSet(btree, query):\n",
    "    \"\"\"\n",
    "    Given a binary partition tree\n",
    "    returns indices of points in the cell that contains query point\n",
    "    \"\"\"\n",
    "    ## base case: return data indices if leaf is reached    \n",
    "    if btree.leftChild_ is None and btree.rightChild_ is None:\n",
    "        return btree.data_ind\n",
    "        \n",
    "    ## check which subset does the query belong to\n",
    "    if np.dot(btree.w_, query) < btree.thres_: \n",
    "        return retrievalSet(btree.leftChild_, query)\n",
    "    else:\n",
    "        return retrievalSet(btree.rightChild_, query)\n",
    "        \n",
    "def getDepth(btree, depth):\n",
    "    \"\"\"\n",
    "    find out the depth (maximal height of all branch) of a binary tree\n",
    "    depth is the depth of current node\n",
    "    \"\"\"\n",
    "    ## via DFS\n",
    "    if btree.leftChild_ is None and btree.rightChild_ is None:\n",
    "        return depth\n",
    "        \n",
    "    depthL = getDepth(btree.leftChild_, depth+1)\n",
    "    depthR = getDepth(btree.rightChild_, depth+1)\n",
    "        \n",
    "    if depthL >= depthR:\n",
    "        return depthL\n",
    "    else:\n",
    "        return depthR\n",
    "        \n",
    "def predict_one_bt(btree, point, predict_type='class'):\n",
    "    assert btree.labels is not None, \"This tree has no associated data labels!\"\n",
    "    set_ind = retrievalSet(btree, point)\n",
    "    \n",
    "    if predict_type == 'class':\n",
    "        if not list(set_ind):\n",
    "            return round(np.mean(btree.labels))\n",
    "        return int(round(np.mean(btree.labels[set_ind])))\n",
    "        \n",
    "    else:\n",
    "        # regression\n",
    "        if not list(set_ind):\n",
    "            return np.mean(btree.labels)\n",
    "        return np.mean(btree.labels[set_ind])\n",
    "        \n",
    "    \n",
    "def predict_bt(btree, test, predict_type='class'):\n",
    "    \"\"\"\n",
    "    Returns a list of predictions corresponding to test set\n",
    "    \"\"\"\n",
    "    predictions = list()\n",
    "    for point in test:\n",
    "        predictions.append(predict_one_bt(btree, point, predict_type=predict_type))\n",
    "    return predictions\n",
    "\n",
    "        \n",
    "def k_nearest(tree, query):\n",
    "    \"\"\"\n",
    "    Given dataset organized in a tree structure  ----- TO BE IMPLEMENTED\n",
    "    find the approximate k nearest neighbors of query point\n",
    "    \"\"\"\n",
    "    return k_nearest\n",
    "        \n",
    "    \n",
    "###\n",
    "def printPartition(tree, level):\n",
    "    \"\"\"\n",
    "    Starting from root of the tree, traverse each node at given level\n",
    "    and print the partitioning it holds\n",
    "    Can be used for test purposes\n",
    "    \"\"\"\n",
    "    if tree.height_ == level or (tree.leftChild_ is None and tree.rightChild_ is None):\n",
    "        ind_set = []\n",
    "        for i in range(len(tree.data_ind)):\n",
    "            if tree.data_ind[i] == 1:\n",
    "                ind_set.append(i)\n",
    "        print(ind_set)\n",
    "    else:\n",
    "        printPartition(tree.leftChild_, level)\n",
    "        printPartition(tree.rightChild_, level)\n",
    "        \n",
    "def traverseLeaves(tree):\n",
    "    \n",
    "    if tree.leftChild_ is None and tree.rightChild_ is None:\n",
    "        yield tree\n",
    "    \n",
    "    if tree.leftChild_ is not None:\n",
    "        for t in traverseLeaves(tree.leftChild_):\n",
    "            yield t\n",
    "    if tree.rightChild_ is not None:\n",
    "        for t in traverseLeaves(tree.rightChild_):\n",
    "            yield t\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######------- A class of master trees inspired by Kpotufe's adaptive tree structure\n",
    "## these are not binary trees but are \"meta-trees\" built on binary trees\n",
    "## it is used to prune a binary tree as we grow it\n",
    "\n",
    "class master_trees(object):\n",
    "    \n",
    "    def __init__(self, data, labels=None,slave_tree=None,\n",
    "                 slave_tree_params={'name':'coreRP','repeat':0}, \n",
    "                 curr_height=0, max_height=5):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.slave_tree_params={'name':'coreRP','repeat':0}\n",
    "        self.curr_height = curr_height\n",
    "        self.max_height = max_height\n",
    "        self.leaves_list = list()\n",
    "        \n",
    "        ## initialize slave tree\n",
    "        if slave_tree is None:\n",
    "            if slave_tree_params['name'] == 'coreRP':\n",
    "                data_ind = np.ones(self.data.shape[0], dtype=bool)\n",
    "                proj_design = {'name':'projmat', 'params':{'name':'dasgupta'}}\n",
    "            \n",
    "                ddiameter,_,_ = data_diameter(self.data)\n",
    "                split_design = {'name':'median_perturb', 'params':{'root_height':curr_height, 'diameter':ddiameter}}\n",
    "                stop_design = {'name':'cell_size', 'diameter':ddiameter}\n",
    "            \n",
    "                self.slave_tree = flex_binary_trees(self.data, data_ind, proj_design=proj_design, \n",
    "                                                split_design=split_design, stop_design=stop_design, \n",
    "                                                height=self.curr_height, labels=self.labels, master_tree=self)\n",
    "        else:\n",
    "            self.slave_tree = slave_tree\n",
    "            self.slave_tree.master_tree = self\n",
    "            \n",
    "            \n",
    "        \n",
    "    def grow_leaves(self):\n",
    "        self.slave_tree.buildtree()\n",
    "        depth = getDepth(self.slave_tree, 0)\n",
    "        \n",
    "        for i in range(self.slave_tree_params['repeat']):\n",
    "            ## select tree with shortest depth\n",
    "            slave_tree = flex_binary_trees(self.data, data_ind, proj_design=proj_design, split_design=split_design, \n",
    "                              stop_design=stop_design, height=self.curr_height, labels=self.labels, master_tree=self)\n",
    "            slave_tree.buildtree()\n",
    "            if depth > getDepth(slave_tree, 0):\n",
    "                self.slave_tree = slave_tree\n",
    "        \n",
    "        \n",
    "    def iter_leaves(self):\n",
    "        return traverseLeaves(self.slave_tree)\n",
    "        \n",
    "    def test_stop(self):\n",
    "        if getDepth(self.slave_tree, self.curr_height) >= self.max_height:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def build_master_trees(self):\n",
    "        self.grow_leaves()\n",
    "        leaves_gen = self.iter_leaves() # up to this point, nodes are binary trees\n",
    "        \n",
    "        for leaf in leaves_gen:\n",
    "            new_master_tree = master_trees(data, labels=self.labels, slave_tree=leaf,\n",
    "                                           slave_tree_params=self.slave_tree_params, \n",
    "                                            curr_height=leaf.height_ ,max_height=self.max_height )\n",
    "            self.leaves_list.append(new_master_tree)\n",
    "            if not self.test_stop():\n",
    "                ## if max height is not achieved in any cell of slave tree\n",
    "                # continue grow it\n",
    "                new_master_tree.build_master_trees()\n",
    "                \n",
    "    def train(self):\n",
    "        ## interface with cross-validation evalutaion\n",
    "        self.build_master_trees()\n",
    "        \n",
    "    def predict_one(self, point, predict_type='class'):\n",
    "        return predict_one_mt(self, point, predict_type=predict_type)\n",
    "     \n",
    "    def predict(self, test, predict_type='class'):\n",
    "        ## interface with cross-validation evaluation\n",
    "        return predict_mt(self, test, predict_type=predict_type)\n",
    "\n",
    "            \n",
    "\n",
    "####----------- Utility functions for master trees\n",
    "\n",
    "def retrievalLeaf_mtree(mtree, query):\n",
    "    ## base case\n",
    "    if not mtree.leaves_list:\n",
    "        return mtree\n",
    "    \n",
    "    ##\n",
    "    slave_leaf = retrievalLeaf(mtree.slave_tree, query) #find the leaf containing query using its binary slave tree\n",
    "    return retrievalLeaf_mtree(slave_leaf.master_tree, query) #recurse on master tree of the found leaf\n",
    "\n",
    "def traverseLeaves_mtree(mtree):\n",
    "    if not mtree.leaves_list:\n",
    "        yield mtree\n",
    "    \n",
    "    for leaf in mtree.iter_leaves():\n",
    "        traverseLeaves_mtree(leaf.master_tree)\n",
    "        \n",
    "        \n",
    "def predict_one_mt(mtree, point, predict_type='class'):\n",
    "    mcell = retrievalLeaf_mtree(mtree, point)\n",
    "    set_ind = mcell.slave_tree.data_ind\n",
    "    \n",
    "    if predict_type == 'class':\n",
    "        #print(round(np.mean(mcell.slave_tree.labels[set_ind])))\n",
    "        if not list(set_ind):\n",
    "            # if set index is an empty list\n",
    "            return round(np.mean(mcell.slave_tree.labels))\n",
    "        return round(np.mean(mcell.slave_tree.labels[set_ind]))\n",
    "        \n",
    "    else:\n",
    "        # regression\n",
    "        if not list(set_ind):\n",
    "            return np.mean(mcell.slave_tree.labels)\n",
    "        return np.mean(mcell.slave_tree.labels[set_ind])\n",
    "    \n",
    "def predict_mt(mtree, test, predict_type='class'):\n",
    "    \"\"\"\n",
    "    Returns a list of predictions corresponding to test set\n",
    "    \"\"\"\n",
    "    predictions = list()\n",
    "    for point in test:\n",
    "        predictions.append(predict_one_mt(mtree, point, predict_type=predict_type))\n",
    "    return predictions        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Some helper functions\n",
    "\n",
    "def subsample(data, n_samples, n_features=None):\n",
    "    \"\"\"\n",
    "    sample WITH replacement\n",
    "    \"\"\"\n",
    "    ind_data = np.random.choice(data.shape[0], size=n_samples, replace=True)\n",
    "    if n_features is not None:\n",
    "        ind_features = np.random.choice(data.shape[1], size=n_features, replace=True)\n",
    "        return data[ind_data, ind_features]\n",
    "        \n",
    "    return data[ind_data,:] \n",
    "\n",
    "def cross_valid_split(n_data, n_folds):\n",
    "    \"\"\"\n",
    "    Given size of the data and number of folds\n",
    "    Returns n_folds disjoint sets of indices, where indices\n",
    "    in each fold are chosen u.a.r. without replacement\n",
    "    \"\"\"\n",
    "    data_ind = range(n_data)\n",
    "    folds = list()\n",
    "    fold_size = floor(n_data/n_folds)\n",
    "    for i in range(n_folds):\n",
    "        if i < n_folds-1:\n",
    "            fold = list()\n",
    "            while len(fold) <= fold_size:\n",
    "                index = random.randrange(len(data_ind))\n",
    "                fold.append(data_ind.pop(index))\n",
    "            folds.append(fold)\n",
    "        else:\n",
    "            ## assign all remaining data to the last fold\n",
    "            folds.append(data_ind)\n",
    "    return folds\n",
    "\n",
    "def zero_one_loss(labels, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == predictions[i]:\n",
    "            correct += 1\n",
    "    loss = (len(labels)-correct)/float(len(labels))\n",
    "    return loss\n",
    "\n",
    "def explained_var_loss(labels, predictions):\n",
    "    #res_var = np.sum(np.array([diff**2 for diff in labels-predictions]))\n",
    "    res_var = np.var(np.array(labels)-np.array(predictions))\n",
    "    tot_var = np.var(np.array(labels))\n",
    "    \n",
    "    return 1-res_var/tot_var\n",
    "\n",
    "def l2_loss(labels, predictions):\n",
    "    loss = np.linalg.norm(np.array(labels)-np.array(predictions))\n",
    "    return loss/(len(labels)**(1/2))\n",
    "\n",
    "def csize_decrease_rate(data, tree):\n",
    "    \"\"\"\n",
    "    This is an unsupervised evaluation, which tries to capture how fast\n",
    "    the data size of a cell decreases after building a tree\n",
    "    \"\"\"\n",
    "    diam_s,_,_ = data_diameter(data)\n",
    "    diam_f = 0\n",
    "    \n",
    "    if hasattr(tree, 'slave_tree'):\n",
    "        ## if this is a master tree\n",
    "        for leaf in traverseLeaves_mtree(tree):\n",
    "            diam, _, _ = data_diameter(leaf.slave_tree.data[leaf.slave_tree.data_ind, :])\n",
    "            if diam > diam_f:\n",
    "                diam_f = diam\n",
    "            \n",
    "    else:\n",
    "        ## if this is normal binary tree\n",
    "        for leaf in traverseLeaves(tree):\n",
    "            diam, _, _ = data_diameter(leaf.data[leaf.data_ind,:])\n",
    "            if diam > diam_f:\n",
    "                diam_f = diam\n",
    "        \n",
    "    return (diam_s/diam_f)/getDepth(tree)\n",
    "        \n",
    "\n",
    "def cross_valid_eval(data, labels, n_folds, loss, algorithm, **kwargs):\n",
    "    \"\"\"\n",
    "    Given data and labels, a loss function, and a method\n",
    "    generate a list of cv-losses\n",
    "    \n",
    "    \"\"\"\n",
    "    ## generate random folds\n",
    "    folds_ind = cross_valid_split(data.shape[0], n_folds)\n",
    "    losses = list()\n",
    "    for fold_ind in folds_ind:\n",
    "        test_ind = fold_ind\n",
    "        folds_ind_ = list(folds_ind) # this ensures we are not modifying the original list!\n",
    "        folds_ind_.remove(fold_ind)\n",
    "        train_ind = [item for sublist in folds_ind_ for item in sublist] #flatten remaining index set\n",
    "        data_tr = data[train_ind,:]\n",
    "        labels_tr = labels[train_ind]\n",
    "        data_tt = data[test_ind,:]\n",
    "        labels_tt = labels[test_ind]\n",
    "        # train the algorithm \n",
    "        alg = algorithm(data_tr, labels=labels_tr, **kwargs) #init\n",
    "        alg.train()\n",
    "        # calculate loss\n",
    "        losses.append(loss(labels_tt, alg.predict(data_tt)))\n",
    "        #print(labels[0],alg.predict(data_tt)[0])\n",
    "        del alg\n",
    "    return losses\n",
    "        \n",
    "\"\"\"\n",
    "Exp\n",
    "\"\"\"\n",
    "\n",
    "class forest(object):\n",
    "    def __init__(self, data, labels=None, tree_type=None, predictor_type='regress',\n",
    "                  n_trees=10, n_samples=100, n_features=None):\n",
    "        \"\"\"\n",
    "        tree_type: A dictionary containing \n",
    "          - a proj_design dictionary, \n",
    "          - a split_design dictionary,\n",
    "          - a stop_rule method\n",
    "        \n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tree_type = tree_type\n",
    "        self.predictor_type = predictor_type\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = list() # store trees for re-use\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        \n",
    "    def reset_sample_size(self, n_samples=None, n_features=None):\n",
    "        if n_samples is not None:\n",
    "            self.n_samples = n_samples\n",
    "        if n_features is not None:\n",
    "            self.n_features = n_features\n",
    "            \n",
    "    def reset_predictor_type(self, method):\n",
    "        self.predictor_type = method\n",
    "        \n",
    "    def build_forest_classic(self, isPredict=True):\n",
    "        if isPredict:\n",
    "            assert labels is not None, \"Data labels missing!\"\n",
    "        tree_type = self.tree_type['tree']\n",
    "        for i in range(self.n_trees):\n",
    "            #sample data points with replacement\n",
    "            data_ind = np.random.choice(self.data.shape[0], self.n_samples, replace=True)\n",
    "            data_tree = self.data[data_ind,:] # data unique to this tree\n",
    "            \n",
    "            if self.n_features is not None:\n",
    "                ## optionally subsample features\n",
    "                feature_ind = np.random.choice(self.data.shape[1], self.n_features, replace=True)\n",
    "                data_tree = data_tree[:,feature_ind] #features unique to this tree\n",
    "            \n",
    "            if isPredict:\n",
    "                labels_tree = self.labels[data_ind]\n",
    "            else:\n",
    "                labels_tree = None\n",
    "            \n",
    "            if tree_type == 'flex':\n",
    "                proj_design, split_design, stop_design = self.tree_type['proj_design'],\\\n",
    "                   self.tree_type['split_design'],self.tree_type['stop_design']\n",
    "                self.trees.append(flex_binary_trees(data_tree, np.ones(data_tree.shape[0], dtype=bool), \n",
    "                                        proj_design,split_design, stop_design, labels=labels_tree))\n",
    "            else:\n",
    "                ## use Kpotufe's adaptive RP tree\n",
    "                self.trees.append(master_trees(data_tree, labels=labels_tree))\n",
    "    \n",
    "    \n",
    "    def build_forest_with_tree_preproc(self, method):\n",
    "        pass\n",
    "        \n",
    "    def build_forest_with_forest_preproc(self, method):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        self.build_forest_classic(isPredict=True)\n",
    "    \n",
    "    def predict_one(self, point):\n",
    "        \"\"\"\n",
    "        Predictor_type can be either 'class' for classification,\n",
    "        'regress' for regression, or a user-defined callable function\n",
    "        \"\"\"\n",
    "        assert self.trees, \"You must first build a forest\"\n",
    "        \n",
    "        if self.predictor_type == 'class':\n",
    "            ## binary classification\n",
    "            avg_predict = 0\n",
    "            for tree in self.trees:\n",
    "                avg_predict += tree.predict_one(point, predict_type='class')\n",
    "            if avg_predict/float(len(self.trees)) > 0.5:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        elif self.predictor_type == 'regress':\n",
    "            ## regression\n",
    "            avg_predict = 0\n",
    "            for tree in self.trees:\n",
    "                avg_predict += tree.predict_one(point, predict_type='regress')\n",
    "            return avg_predict/float(len(self.trees))\n",
    "        else:\n",
    "            print(\"Unrecognized prediction method!\")\n",
    "           \n",
    "    def predict(self, test):\n",
    "        predictions = list()\n",
    "        for point in test:\n",
    "            predictions.append(self.predict_one(point))\n",
    "        return predictions\n",
    "                \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function: Get data from url link and store\n",
    "def download_data(fname, url_name, force = False):\n",
    "    if force or not os.path.exists(fname):\n",
    "        print('Downloading data from the internet...')\n",
    "        try:\n",
    "            urllib.urlretrieve(url_name, fname)\n",
    "        except Exception as e:\n",
    "            print(\"Unable to retrieve file from given url\")\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "# Utility function: pickle or get pickled data with desired dataname\n",
    "def maybe_pickle(dataname, data = None, force = False, verbose = True):\n",
    "    \"\"\"\n",
    "    Process and pickle a dataset if not present\n",
    "    \"\"\"\n",
    "    filename = dataname + '.pickle'\n",
    "    if force or not os.path.exists(filename):\n",
    "        # pickle the dataset\n",
    "        print('Pickling data to file %s' % filename)\n",
    "        try:\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save to', filename, ':', e) \n",
    "    else:\n",
    "        print('%s already present - Skipping pickling.' % filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "## Download sonar dataset for classification problems\n",
    "fname = \"sonar.all_data.csv\"\n",
    "url_name = \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "download_data(fname, url_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data has shape 208 by 60\n",
      "Preprocessed labels has length 208\n",
      "sonar.all_data.pickle already present - Skipping pickling.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.0151,  0.032 ,  0.0599, ...,  0.0019,  0.0023,  0.0062],\n",
       "        [ 0.0336,  0.0294,  0.0476, ...,  0.0015,  0.0069,  0.0051],\n",
       "        [ 0.0195,  0.0213,  0.0058, ...,  0.0095,  0.0021,  0.0053],\n",
       "        ..., \n",
       "        [ 0.0519,  0.0548,  0.0842, ...,  0.0047,  0.0048,  0.0053],\n",
       "        [ 0.1371,  0.1226,  0.1385, ...,  0.0079,  0.0146,  0.0051],\n",
       "        [ 0.0089,  0.0274,  0.0248, ...,  0.0069,  0.006 ,  0.0018]]),\n",
       " 'labels': array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,\n",
       "         1.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "         1.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
       "         1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
       "         0.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,\n",
       "         0.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,\n",
       "         1.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,\n",
       "         1.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "         1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,\n",
       "         1.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,\n",
       "         1.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### module for preprocessing raw data and pickling it\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    " \n",
    "# Convert string column to float\n",
    "# this is for data conversion\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string\n",
    "# this is for label conversion\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "\n",
    "### load and preprocess data\n",
    "fname = 'sonar.all_data.csv'\n",
    "data = load_csv(fname)\n",
    "for i in range(len(data[0])-1):\n",
    "    str_column_to_float(data, i)\n",
    "    \n",
    "lookup = str_column_to_int(data, -1)\n",
    "data = np.array(data)\n",
    "## random shuffling\n",
    "pInd = np.random.permutation(data.shape[0])\n",
    "data = data[pInd]\n",
    "X = data[:,:data.shape[-1]-1]\n",
    "labels = data[:, -1]\n",
    "print(\"Preprocessed data has shape %d by %d\" % X.shape)\n",
    "print(\"Preprocessed labels has length %d\" % len(labels))\n",
    "## pickle data\n",
    "maybe_pickle('sonar.all_data', {'data':X, 'labels':labels}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sonar.all_data.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "### Test: tree\n",
    "data_dict = maybe_pickle('sonar.all_data')\n",
    "data = data_dict['data']\n",
    "labels = data_dict['labels']\n",
    "#data_ind = np.ones(data.shape[0], dtype=bool)\n",
    "scores = cross_valid_eval(data, labels, 5, zero_one_loss, flex_binary_trees)\n",
    "score1 = np.mean(scores)\n",
    "#testTree = flex_binary_trees(data, data_ind , proj_design, split_design, stop_design, 0, labels=labels)\n",
    "#testTree.buildtree()\n",
    "\n",
    "\n",
    "#count = 0\n",
    "#for leaf in traverseLeaves(testTree):\n",
    "#    count += 1\n",
    "#    print(\"Depth of the %d th leaf is %d\" %(count, leaf.height_))\n",
    "#print(\"Depth of the tree is %d\" %testTree.getDepth(0))\n",
    "#query = data[1,:]\n",
    "#rset = testTree.retrievalSet(query)\n",
    "#print(\"size of retrieved set is %d\" %np.sum(rset))\n",
    "#printPartition(testTree, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sonar.all_data.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "## Test: forest + breiman-tree + cart + naive stopping\n",
    "data_dict = maybe_pickle('sonar.all_data')\n",
    "data = data_dict['data']\n",
    "labels = data_dict['labels']\n",
    "proj_design={'name':'projmat','params':{'name':'breiman','sparsity':3,'target_dim':10}}\n",
    "split_design={'name':'cart', 'params':{'root_height':0, 'diameter':ddiameter}} \n",
    "stop_design={'name':'naive'}\n",
    "kwargs = {'tree_type':{\"tree\":'flex','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'class','n_trees':100, 'n_samples':50}\n",
    "\n",
    "scores = cross_valid_eval(data, labels, 5, zero_one_loss, forest, **kwargs)\n",
    "score2 = np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sonar.all_data.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "### Test adaptive RP-tree\n",
    "data_dict = maybe_pickle('sonar.all_data')\n",
    "data = data_dict['data']\n",
    "labels = data_dict['labels']\n",
    "labels_ind = np.isnan(labels)\n",
    "mtree = master_trees(data)\n",
    "#mtree.build_master_trees()\n",
    "count=0\n",
    "#for leaf in traverseLeaves(mtree.slave_tree):\n",
    "#    count += 1\n",
    "#    print(\"Depth of the %d th leaf is %d\" %(count, leaf.height_))\n",
    "#    print(leaf.master_tree.curr_height)\n",
    "#getDepth(mtree.slave_tree,0)\n",
    "scores = cross_valid_eval(data, labels, 5, zero_one_loss, master_trees)\n",
    "score3 = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sonar.all_data.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "## Test: forest+ adaptiveRP\n",
    "data_dict = maybe_pickle('sonar.all_data')\n",
    "data = data_dict['data']\n",
    "labels = data_dict['labels']\n",
    "data_ind = np.ones(data.shape[0], dtype=bool)\n",
    " \n",
    "kwargs = {'tree_type':{\"tree\":'master'},'predictor_type':'class','n_trees':100, 'n_samples':50}\n",
    "\n",
    "scores = cross_valid_eval(data, labels, 5, zero_one_loss, forest, **kwargs)\n",
    "score4 = np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test: forest + naive RP\n",
    "proj_design={'name':'projmat','params':{'name':'dasgupta'}}\n",
    "split_design={'name':'median_perturb', 'params':{'regress':False}}\n",
    "stop_design={'name':'naive'}\n",
    "kwargs = {'tree_type':{\"tree\":'flex','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'class','n_trees':100, 'n_samples':50}\n",
    "\n",
    "scores = cross_valid_eval(data, labels, 5, zero_one_loss, forest, **kwargs)\n",
    "score5 = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4319047619047619,\n",
       " 0.46500000000000002,\n",
       " 0.33738095238095239,\n",
       " 0.4659523809523809,\n",
       " 0.46619047619047621)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1,score2,score3,score4,score5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My observations from preliminary experiments:\n",
    "- Forest type estimators seem to perform uniformly worse than single-tree based estimators on sonar data\n",
    "- Adaptive RP tree seems to perform significantly better than others, though it takes a lot more time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
