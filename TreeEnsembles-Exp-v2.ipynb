{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pylab\n",
    "####################### Plotting tools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn\n",
    "####################### data frames\n",
    "import pandas as pd\n",
    "from pandas.tools import plotting\n",
    "####################### Utilities\n",
    "import urllib\n",
    "from six.moves import cPickle as pickle\n",
    "import random \n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "from math import floor\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pandas import DataFrame\n",
    "from IPython.display import display\n",
    "######################## From scikit-learn\n",
    "from sklearn import random_projection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "######################## From home directory\n",
    "from forest_class import forest\n",
    "from generalTrees_class import flex_binary_trees, master_trees, getDepth\n",
    "from tree_utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Uniform distribution over hypercube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In uniform cube, there are 8000 train and 2000 test data\n"
     ]
    }
   ],
   "source": [
    "def sample_uniform_hypercube(n_samples, dimension):\n",
    "    \"\"\"\n",
    "    Returns sample size by dimension\n",
    "    \"\"\"\n",
    "    def map_function(null):\n",
    "        return np.random.uniform(low=0.0, high=1.0, size=n_samples)\n",
    "        \n",
    "    return np.transpose(np.array(list(map(map_function, [None]*dimension))))\n",
    "\n",
    "## test\n",
    "#sample_uniform_hypercube(10, 5)\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "dimension = 10\n",
    "n_samples = 10000\n",
    "data = sample_uniform_hypercube(n_samples, dimension)\n",
    "mu = 5\n",
    "sigma = 2\n",
    "randmat = sigma*np.random.randn(10)+mu ## random normal\n",
    "labels = np.dot(data, randmat) ## labels generated by a simple linear model\n",
    "data_tr = data[:8000,:]\n",
    "labels_tr = labels[:8000]\n",
    "data_tt = data[8000:,:]\n",
    "labels_tt = labels[8000:]\n",
    "print('In uniform cube, there are %d train and %d test data' %(len(labels_tr), len(labels_tt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######------ Utility functions\n",
    "def cross_valid_split(n_data, n_folds):\n",
    "    \"\"\"\n",
    "    Given size of the data and number of folds\n",
    "    Returns n_folds disjoint sets of indices, where indices\n",
    "    in each fold are chosen u.a.r. without replacement\n",
    "    \"\"\"\n",
    "    data_ind = list(range(n_data))    \n",
    "    folds = list()\n",
    "    fold_size = floor(n_data/n_folds)\n",
    "    for i in range(n_folds):\n",
    "        if i < n_folds-1:\n",
    "            fold = list()\n",
    "            while len(fold) <= fold_size:\n",
    "                index = random.randrange(len(data_ind))\n",
    "                fold.append(data_ind.pop(index))\n",
    "            folds.append(fold)\n",
    "        else:\n",
    "            ## assign all remaining data to the last fold\n",
    "            folds.append(data_ind)\n",
    "    return folds\n",
    "\n",
    "def zero_one_loss(labels, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == predictions[i]:\n",
    "            correct += 1\n",
    "    loss = (len(labels)-correct)/float(len(labels))\n",
    "    return loss\n",
    "\n",
    "def explained_var_loss(labels, predictions):\n",
    "    #res_var = np.sum(np.array([diff**2 for diff in labels-predictions]))\n",
    "    res_var = np.var(np.array(labels)-np.array(predictions))\n",
    "    tot_var = np.var(np.array(labels))\n",
    "    \n",
    "    return 1-res_var/tot_var\n",
    "\n",
    "def l2_loss(labels, predictions):\n",
    "    loss = np.linalg.norm(np.array(labels)-np.array(predictions))\n",
    "    return loss/(len(labels)**(1/2))\n",
    "\n",
    "def csize_decrease_rate(data, tree):\n",
    "    \"\"\"\n",
    "    This is an unsupervised evaluation, which tries to capture how fast\n",
    "    the data size of a cell decreases after building a tree\n",
    "    \"\"\"\n",
    "    diam_s,_,_ = data_diameter(data)\n",
    "    diam_f = 0\n",
    "    \n",
    "    if hasattr(tree, 'slave_tree'):\n",
    "        ## if this is a master tree\n",
    "        for leaf in traverseLeaves_mtree(tree):\n",
    "            diam, _, _ = data_diameter(leaf.slave_tree.data[leaf.slave_tree.data_ind, :])\n",
    "            if diam > diam_f:\n",
    "                diam_f = diam\n",
    "            \n",
    "    else:\n",
    "        ## if this is normal binary tree\n",
    "        for leaf in traverseLeaves(tree):\n",
    "            diam, _, _ = data_diameter(leaf.data[leaf.data_ind,:])\n",
    "            if diam > diam_f:\n",
    "                diam_f = diam\n",
    "        \n",
    "    return (diam_s/diam_f)/getDepth(tree)\n",
    "        \n",
    "\n",
    "def cross_valid_eval(data, labels, n_folds, loss, algorithm, sklearn=False, need_ind=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Given data and labels, a loss function, and a method\n",
    "    generate a list of cv-losses\n",
    "    \n",
    "    \"\"\"\n",
    "    ## generate random folds\n",
    "    folds_ind = cross_valid_split(data.shape[0], n_folds)\n",
    "    losses = list()\n",
    "    for fold_ind in folds_ind:\n",
    "        print(\"Evaluating the %d-th fold\" %(len(losses)+1))\n",
    "        test_ind = fold_ind\n",
    "        folds_ind_ = list(folds_ind) # this ensures we are not modifying the original list!\n",
    "        folds_ind_.remove(fold_ind)\n",
    "        train_ind = [item for sublist in folds_ind_ for item in sublist] #flatten remaining index set\n",
    "        ## Further divide the data into train and test\n",
    "        data_tr = data[train_ind,:]\n",
    "        labels_tr = labels[train_ind]\n",
    "        data_tt = data[test_ind,:]\n",
    "        labels_tt = labels[test_ind]\n",
    "        # train the algorithm \n",
    "        if sklearn:\n",
    "            alg = algorithm(**kwargs)\n",
    "            alg.fit(data_tr, labels_tr)\n",
    "        elif need_ind:\n",
    "            data_ind = range(len(data_tr))\n",
    "            alg = algorithm(data_tr, data_indices=data_ind, labels=labels_tr, **kwargs) #init\n",
    "            alg.train()\n",
    "        else:\n",
    "            # RF and master trees don't need to be given an index \n",
    "            alg = algorithm(data_tr, labels=labels_tr, **kwargs) #init\n",
    "            alg.train()\n",
    "        #c = print_mtree_leaves(alg) ## added should be removed\n",
    "        #print(\"There are %d partitions\" %c)\n",
    "        \n",
    "        # calculate loss on the current fold\n",
    "        #dp = None\n",
    "        #if isinstance(alg, DecisionTreeClassifier):\n",
    "        #    dp = alg.decision_path(data_tr)\n",
    "        losses.append(loss(labels_tt, alg.predict(data_tt)))\n",
    "        print('Generated tree with height', getDepth(alg,0))\n",
    "        #print(labels[0],alg.predict(data_tt)[0])\n",
    "        del alg\n",
    "    return losses           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base tree evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.000000\n",
      "Training loss: 171.673693\n"
     ]
    }
   ],
   "source": [
    "######## Median Tree ############\n",
    "proj_design={'name':'projmat','params':{'name':'breiman','sparsity':1,'target_dim':1}}  \n",
    "split_design = {'name':'median'}\n",
    "stop_design={'name':'naive'}\n",
    "kwargs = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design, 'predict_type':'regress'}\n",
    "scores_c1 = list()\n",
    "#scores_c1_test = list()\n",
    "#params_c1 = list()\n",
    "data_ind = range(len(data_tr))\n",
    "alg = flex_binary_trees(data_tr, data_indices=data_ind, labels=labels_tr, **kwargs)\n",
    "alg.train()\n",
    "predict_on_train = alg.predict(data_tr)\n",
    "print('Training loss: %f' %l2_loss(labels_tr, predict_on_train))\n",
    "predict_on_test = alg.predict(data_tt)\n",
    "print('Test loss: %f' %l2_loss(labels_tt, predict_on_test))\n",
    "\n",
    "##\n",
    "# scores_c1.append(cross_valid_eval(data_tr, labels_tr, 4, l2_loss, flex_binary_trees, \n",
    "#                                               need_ind=True, **kwargs))\n",
    "# scores_c1_test.append(cross_valid_eval(data_tt, labels_tt, 5, zero_one_loss, flex_binary_trees, \n",
    "#     #                                          need_ind=True, **kwargs))\n",
    "#params_c1.append([t_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[190.81450214753312,\n",
       "  187.89859330078139,\n",
       "  184.2559363676277,\n",
       "  173.38674069948627]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_c1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.000000\n",
      "Test loss: 113.065578\n"
     ]
    }
   ],
   "source": [
    "######## Median Spill Tree ############\n",
    "proj_design={'name':'projmat','params':{'name':'breiman','sparsity':1,'target_dim':1}}  \n",
    "split_design = {'name':'median_spill'}\n",
    "stop_design={'name':'naive'}\n",
    "kwargs = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design, 'predict_type':'regress'}\n",
    "data_ind = range(len(data_tr))\n",
    "alg = flex_binary_trees(data_tr, data_indices=data_ind, labels=labels_tr, **kwargs)\n",
    "alg.train()\n",
    "predict_on_train = alg.predict(data_tr)\n",
    "print('Training loss: %f' %l2_loss(labels_tr, predict_on_train))\n",
    "predict_on_test = alg.predict(data_tt)\n",
    "print('Test loss: %f' %l2_loss(labels_tt, predict_on_test))\n",
    "#scores_mspill = list()\n",
    "#scores_c1_test = list()\n",
    "#params_c1 = list()\n",
    "\n",
    "##\n",
    "# scores_mspill.append(cross_valid_eval(data_tr, labels_tr, 4, l2_loss, flex_binary_trees, \n",
    "#                                               need_ind=True, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[107.51614968978102,\n",
       "  125.00119234989374,\n",
       "  116.91040888219152,\n",
       "  118.08578107669975]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_mspill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.000000\n",
      "Test loss: 103.683036\n"
     ]
    }
   ],
   "source": [
    "######## Randomized Spill Tree ############\n",
    "proj_design={'name':'projmat','params':{'name':'dasgupta','target_dim':1}}  \n",
    "split_design = {'name':'median_spill'}\n",
    "stop_design={'name':'naive'}\n",
    "kwargs = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design, 'predict_type':'regress'}\n",
    "data_ind = range(len(data_tr))\n",
    "alg = flex_binary_trees(data_tr, data_indices=data_ind, labels=labels_tr, **kwargs)\n",
    "alg.train()\n",
    "predict_on_train = alg.predict(data_tr)\n",
    "print('Training loss: %f' %l2_loss(labels_tr, predict_on_train))\n",
    "predict_on_test = alg.predict(data_tt)\n",
    "print('Test loss: %f' %l2_loss(labels_tt, predict_on_test))\n",
    "#scores_rpspill = list()\n",
    "#scores_c1_test = list()\n",
    "#params_c1 = list()\n",
    "\n",
    "##\n",
    "# scores_rpspill.append(cross_valid_eval(data_tr, labels_tr, 4, l2_loss, flex_binary_trees, \n",
    "#                                               need_ind=True, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Forest level parameters\n",
    "n_trees_list = [10,50,100]\n",
    "#n_samples_list = [10, 50, 100]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.000000\n",
      "Test loss: 101.691774\n",
      "Training loss: 0.000000\n",
      "Test loss: 85.794812\n",
      "Training loss: 0.000000\n",
      "Test loss: 84.410717\n"
     ]
    }
   ],
   "source": [
    "######## Base tree: median Tree ############\n",
    "proj_design={'name':'projmat','params':{'name':'breiman','sparsity':1,'target_dim':1}}  \n",
    "split_design = {'name':'median'}\n",
    "stop_design={'name':'naive'}\n",
    "####\n",
    "kwargs = {'tree_design':{\"tree\":'flex','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'regress'}\n",
    "####\n",
    "for n_trees in n_trees_list:\n",
    "    kwargs['n_trees'] = n_trees\n",
    "    fc_estimator = forest(data_tr, labels=labels_tr, **kwargs)\n",
    "    fc_estimator.train()\n",
    "    predict_on_train = fc_estimator.predict(data_tr)\n",
    "    print('Training loss: %f' %l2_loss(labels_tr, predict_on_train))\n",
    "    predict_on_test = fc_estimator.predict(data_tt)\n",
    "    print('Test loss: %f' %l2_loss(labels_tt, predict_on_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
