{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest playground \n",
    "## My goals are\n",
    "- Explore RF with tranditional decision trees (kd-trees)\n",
    "- Explore RF approach with RP-trees\n",
    "- Explore RF with oblique splits (PCA, LDA, etc)\n",
    "- Explore RF with preprocessings\n",
    " * Random rotation (Vempala)\n",
    " * Randomer forest\n",
    " * Output space random projections\n",
    "\n",
    "Building of random forest (bagging technique) is adapted from tutorial: http://machinelearningmastery.com/implement-random-forest-scratch-python/\n",
    "and part of the design of my spatial-tree class is inspired by \"spatialtree\" package:https://github.com/bmcfee/spatialtree\n",
    "(note: I believe my design is better than that of Brian McFee, since it is more concise and flexible)\n",
    "\n",
    "On sources of data compression:\n",
    " - Forest has three sources of data compression, which may or may not involve randomness: \n",
    "  * preprocessing such as PCA, random projection, random rotation, etc, which can be applied either for a single tree or for the entire forest and achieves\n",
    "    - feature selection/extraction for the entire forest\n",
    "      * This includes the theoretically justified randomly oriented kd-tree in [Vempala 12]\n",
    "    - feature selection/extraction for a single tree\n",
    "  * data subsampling for each tree, which adds randomness\n",
    "  * feature selection/extraction at each tree node (before splitting direction and value are determined)\n",
    "\n",
    " - A generalized forest where methods to achieve compression vary at the node level in [Tomita, Maggioni, Vogelstein 16], where at each step a p by d projection matrix A is chosen; this subsumes\n",
    "  * Forest-IC, Forest-RC in Breiman: sparsity constrained projection matrix sampling\n",
    "  * Rotation forest: deterministic projection matrix via PCA\n",
    "  * Randomer forest: sparse projection matrix via \"very sparse random projections\" \n",
    "  * RP-tree proposed in Dasgupta can also be viewed as a special case here, where the matrix A is a 1-D dense or sparse projection vector\n",
    "(note: when d>1, after projection, the splitting rule must be able to pick a coordinate for splitting, so the first three approaches are more suitable for supervised tasks where coordinate-selection can be made via labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from six.moves import cPickle as pickle\n",
    "import random \n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "from math import floor\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from sklearn import random_projection\n",
    "from pandas import DataFrame\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###---------- Utility functions\n",
    "## computing spread of one-dim data\n",
    "def spread_1D(data_1D):\n",
    "    if len(data_1D)==0:\n",
    "        return 0\n",
    "    return np.amax(data_1D)-np.amin(data_1D)\n",
    "\n",
    "## computing data diamter of a cell\n",
    "def data_diameter(data):\n",
    "    \"\"\"\n",
    "    Input data is assumed to be confined in the\n",
    "    desired cell\n",
    "    \"\"\"\n",
    "    dist, indx, indy = 0, 0, 0\n",
    "    if data.shape[0] == 1:\n",
    "        return dist, indx, indy\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(i+1, data.shape[0]):\n",
    "            dist_new = np.linalg.norm(data[i,:]-data[j,:])\n",
    "            if dist_new > dist:\n",
    "                dist, indx, indy = dist_new, i, j\n",
    "    return dist, indx, indy\n",
    "\n",
    "###---------- compressive projection matrix designs\n",
    "def comp_projmat(data, **kwargs):\n",
    "    \"\"\"\n",
    "    returns a projection matrix\n",
    "    Warning: the projection matrix returned can be either dense or sparse\n",
    "    \"\"\"\n",
    "    namelist = ['breiman','ho','tomita', 'dasgupta']\n",
    "    assert kwargs['name'] in namelist, \"No such method for constructing projection matrix!\"\n",
    "    \n",
    "    if kwargs['name'] == 'breiman':\n",
    "        ## Breiman's Forest-IC and Forest-RC\n",
    "        s = kwargs['sparsity']\n",
    "        d = kwargs['target_dim']\n",
    "        A = np.zeros((data.shape[1],d))\n",
    "        ## sample sparsity-constrained A\n",
    "        for i in range(d):\n",
    "            ind = np.random.choice(data.shape[1], size=s, replace=False)\n",
    "            if s == 1:\n",
    "                A[ind,i] = 1\n",
    "            else:\n",
    "                for j in range(len(ind)):\n",
    "                    A[ind[j], i] = np.random.uniform(-1,1)\n",
    "    \n",
    "    elif kwargs['name'] == 'ho':\n",
    "        ## rotation forest\n",
    "        d = kwargs['target_dim']\n",
    "        ## find A by PCA\n",
    "    elif kwargs['name'] == 'tomita':\n",
    "        ## randomer forest\n",
    "        d = kwargs['target_dim']\n",
    "        ## sample sparse A via very sparse rp\n",
    "        density = 1/(data.shape[1]**(1/2)) #default density value\n",
    "        if 'density' in kwargs:\n",
    "            if kwargs['density'] <= 1 and kwargs['density']>0:\n",
    "                density = kwargs['density']\n",
    "        \n",
    "        transformer = random_projection.SparseRandomProjection(n_components=d, density=density)    \n",
    "        transformer.fit(data)\n",
    "        A = transformer.components_.copy()\n",
    "        A = A.T ## A is SPARSE!\n",
    "        \n",
    "    else:\n",
    "        ## dasgupta rp-tree \n",
    "        d = 1 # default to a random vector          \n",
    "        if 'target_dim' in kwargs:\n",
    "            d = kwargs['target_dim']\n",
    "        n_features = data.shape[1]\n",
    "        A = np.zeros((data.shape[1], d))\n",
    "        # sample dense projection matrix\n",
    "        for i in range(d):\n",
    "            A[:,i] = np.random.normal(0, 1/np.sqrt(n_features), n_features)\n",
    "\n",
    "    return A\n",
    "\n",
    "#######-------split designs\n",
    "\n",
    "## cart splits\n",
    "def cart_split(data, proj_mat, labels=None, regress=False):\n",
    "    # test for the best split feature and threshold on data CART criterion\n",
    "    if sps.issparse(proj_mat):\n",
    "        #data_trans = sps.csr_matrix.dot(data, proj_mat).squeeze()\n",
    "        data_trans = proj_mat.T.dot(data.T).T.squeeze()\n",
    "    else:\n",
    "        data_trans = np.dot(data, proj_mat) # n-by-d\n",
    "\n",
    "    score, ind, thres = -999, None, None\n",
    "    if data_trans.ndim == 1:\n",
    "        if not regress:\n",
    "            #classification\n",
    "            score, thres = cscore(data_trans, labels)\n",
    "        else:\n",
    "            #regression\n",
    "            score, thres = rscore(data_trans, labels)\n",
    "        w = proj_mat\n",
    "    \n",
    "    else:\n",
    "       \n",
    "        for i in range(proj_mat.shape[1]):\n",
    "            if not regress:\n",
    "                # classification\n",
    "                score_new, thres_new = cscore(data_trans[:,i], labels)\n",
    "            else:\n",
    "                # regression\n",
    "                score_new, thres_new = rscore(data_trans[:,i], labels)\n",
    "            if score_new > score:\n",
    "                score = score_new\n",
    "                ind = i\n",
    "                thres = thres_new\n",
    "        w = proj_mat[:,ind]\n",
    "    return score, w, thres\n",
    "\n",
    "def cscore(data_1D, labels):\n",
    "    ## cart classification criterion\n",
    "    score, thres = -999, None\n",
    "    if not list(labels):\n",
    "        return score, thres\n",
    "    n = len(labels)\n",
    "    p1 = np.mean(labels)\n",
    "    data_sorted, ind_sorted = np.sort(data_1D), np.argsort(data_1D)\n",
    "    for i in range(1,n):\n",
    "        cell_l = ind_sorted[:i]\n",
    "        cell_r = ind_sorted[i:]\n",
    "        split_val = data_sorted[i]\n",
    "        if not list(labels[cell_l]) or not list(labels[cell_r]):\n",
    "            #Do nothing if after either of the left and right labels are empty\n",
    "            pass\n",
    "        else:\n",
    "            p1_l = np.mean(labels[cell_l])\n",
    "            p1_r = np.mean(labels[cell_r])\n",
    "            n_l = len(cell_l)\n",
    "            score_new = p1*(1-p1) - (n_l/n)*(1-p1_l)*p1_l - (n-n_l)/n*(1-p1_r)*p1_r  \n",
    "            if score_new > score:\n",
    "                score = score_new\n",
    "                thres = split_val\n",
    "    return score, thres\n",
    "\n",
    "def rscore(data_1D, labels):\n",
    "    ## cart regression criterion\n",
    "    score, thres = -999, None\n",
    "    if not list(labels):\n",
    "        return score, thres\n",
    "    n = len(labels)\n",
    "    ybar = np.mean(labels)\n",
    "    data_sorted, ind_sorted = np.sort(data_1D), np.argsort(data_1D)\n",
    "    \n",
    "    for i in range(1,n):\n",
    "        cell_l = ind_sorted[:i]\n",
    "        cell_r = ind_sorted[i:]\n",
    "        split_val = data_sorted[i]\n",
    "        if not list(labels[cell_l]) or not list(labels[cell_r]):\n",
    "            #Do nothing if after either of the left and right labels are empty\n",
    "            pass\n",
    "        else:\n",
    "            ybar_l = np.mean(labels[cell_l])\n",
    "            ybar_r = np.mean(labels[cell_r])\n",
    "            score_new =(np.sum((labels-ybar)**2)-np.sum((labels[cell_l]-ybar_l)**2)\\\n",
    "                        -np.sum((labels[cell_r]-ybar_r)**2))/n\n",
    "            if score_new > score:\n",
    "                score = score_new\n",
    "                thres = split_val\n",
    "    return score, thres\n",
    "\n",
    "##### median splits\n",
    "\n",
    "def median_split(data, proj_mat, labels=None):\n",
    "    if sps.issparse(proj_mat):\n",
    "        #data_transformed = sps.csr_matrix.dot(data, proj_mat).squeeze()\n",
    "        data_trans = proj_mat.T.dot(data.T).T.squeeze()\n",
    "    else:\n",
    "        data_trans = np.dot(data, proj_mat) # n-by-d\n",
    "    if data_trans.ndim > 1:\n",
    "        score, ind = 0, 0\n",
    "        for i in range(proj_mat.shape[1]):\n",
    "            score_new = spread_1D(data_trans[:,i])\n",
    "            if score_new > score:\n",
    "                score, ind = score_new, i\n",
    "        w = proj_mat[:, ind]\n",
    "        thres = np.median(data_trans[:,ind])\n",
    "    else:\n",
    "        thres = np.median(data_trans)\n",
    "        w = proj_mat\n",
    "        score = spread_1D(data_trans)\n",
    "    return score, w, thres\n",
    "\n",
    "def median_perturb_split(data, proj_mat, node_height, labels=None, diameter=None):\n",
    "\n",
    "    if (node_height) % 2 == 0:\n",
    "        # normal median splits\n",
    "        return median_split(data, proj_mat, labels=labels)\n",
    "    else:\n",
    "        assert diameter is not None, \"Diameter of the cell must be given!\"\n",
    "        # noisy splits\n",
    "        if sps.issparse(proj_mat):\n",
    "            #data_transformed = sps.csr_matrix.dot(data, proj_mat).squeeze()\n",
    "            data_trans = proj_mat.T.dot(data.T).T.squeeze()\n",
    "        else:\n",
    "            data_trans = np.dot(data, proj_mat) # n-by-d\n",
    "        \n",
    "        if data_trans.ndim > 1:\n",
    "            ## We use spread as the way to choose the best feature (could be changed)\n",
    "            score, ind = 0, 0\n",
    "            for i in range(proj_mat.shape[1]):\n",
    "                score_new = spread_1D(data_trans[:, i])\n",
    "                if score_new > score:\n",
    "                    score, ind = score_new, i\n",
    "            w = proj_mat[:, ind]\n",
    "            jitter = np.random.uniform(-1,1) * 6/np.sqrt(data.shape[1]) * diameter\n",
    "            thres = np.median(data_trans[:,ind])+jitter\n",
    "        \n",
    "        else:\n",
    "            jitter = np.random.uniform(-1,1) * 6/np.sqrt(data.shape[1]) * diameter\n",
    "            thres = np.median(data_trans)+jitter\n",
    "            w = proj_mat\n",
    "            score = spread_1D(data_trans)\n",
    "        \n",
    "    \n",
    "        return score, w, thres\n",
    "\n",
    "## 2-means split\n",
    "def two_means_split(data, proj_mat, labels=None):\n",
    "    # this essentially defines a hierarchical clustering on data\n",
    "    return score, w, thres\n",
    "\n",
    "#######-------stop rules\n",
    "def naive_stop_rule(data, height=None):\n",
    "    if data.shape[0] <= 1:\n",
    "        return True\n",
    "    if height > 8:\n",
    "        ## DO NOT ever make it exceed 15!!!\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def cell_size_rule(data, height, max_height=None, target_diameter=None):\n",
    "    ddiameter,_,_ = data_diameter(data)\n",
    "    if target_diameter < 0.01:\n",
    "        return True\n",
    "    if ddiameter <= target_diameter:\n",
    "        return True\n",
    "    elif max_height is not None and height > max_height:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## A class of binary spatial-trees \n",
    "        \n",
    "class flex_binary_trees(object):\n",
    "    \"\"\"\n",
    "    A recursive data structure based on binary trees\n",
    "     - at each node, it contains data, left, right child (or none if leaf), just as any other binary tree\n",
    "     - it also knows its height\n",
    "     - additionally, it has meta information about split direction and split threshold\n",
    "     - to incorporate the use of master-slave trees (see below), it also has an optional reference\n",
    "      to a master tree\n",
    "     \n",
    "    On splitting method\n",
    "     - if rpart or cpart are used, labels must be provided\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, data_indices=None, \n",
    "                 proj_design={'name':'projmat','params':{'name':'breiman','sparsity':3,'target_dim':10}}, \n",
    "                 split_design={'name':'cart', 'params':{'regress':False}}, \n",
    "                 stop_design={'name':'naive'}, \n",
    "                 height=0, labels=None, master_tree=None):\n",
    "        \"\"\"\n",
    "        data: n by d matrix, the entire dataset assigned to the tree\n",
    "        data_indices: the subset of indices assigned to this node\n",
    "        proj_design: A dictionary that contains name and params of a method; \n",
    "          the method returns one or more splitting directions (projection matrix)\n",
    "        split_design: A dict that contains name and params of a function;\n",
    "          the function s.t. given 1D projected data, it must return the splitting threshold\n",
    "        stop_rule: a boolean function of data_indices and height\n",
    "        height: height of current node (root has 0 height)\n",
    "        \"\"\"\n",
    "        assert data_indices is not None, \"You must pass data indices to root!\"\n",
    "        self.data = data\n",
    "        self.data_ind = data_indices\n",
    "        ## This default assignment will cause problems in recursion!\n",
    "        #if data_indices is None:\n",
    "        #    self.data_ind = np.ones(data.shape[0], dtype=bool)\n",
    "        self.proj_design = proj_design\n",
    "        self.split_design = split_design \n",
    "        self.stop_design = stop_design\n",
    "        \n",
    "        self.height_ = height\n",
    "        self.labels = labels\n",
    "        self.master_tree = master_tree\n",
    "        self.leftChild_ = None\n",
    "        self.rightChild_ = None\n",
    "            \n",
    "            \n",
    "        ## set stop rule: a boolean function\n",
    "        if np.sum(self.data_ind) == 0:\n",
    "            ## stop if this cell has no data\n",
    "            self.isLeaf = True\n",
    "        elif self.stop_design['name'] == 'naive':\n",
    "            self.isLeaf = naive_stop_rule(self.data[self.data_ind,:], height=self.height_) \n",
    "            \n",
    "        elif self.stop_design['name'] == 'cell_size':\n",
    "            assert 'params' in self.stop_design, \"Please specify stopping parameters!\"\n",
    "            d0 = self.stop_design['params']['diameter']\n",
    "            max_h = None\n",
    "            if 'max_level' in self.stop_design['params']:\n",
    "                max_h = self.stop_design['params']['max_level']\n",
    "            self.isLeaf = cell_size_rule(self.data[self.data_ind,:], self.height_,\n",
    "                                         max_height=max_h, target_diameter=0.5*d0)\n",
    "        else:\n",
    "            print(\"You must provide a known stopping method!\")\n",
    "            self.isLeaf = True\n",
    "            \n",
    "    \n",
    "    def proj_rule_function(self):\n",
    "        \"\"\"\n",
    "        A function such that given name of a method, returns a projection vector (splitting direction)\n",
    "        Can be override (user-defined)\n",
    "        returns a n_features by n_projected_dim projection matrix, A\n",
    "        \n",
    "        Warning: Should only be executed if self.data_ind has at least ONE nonzero element\n",
    "        \"\"\"\n",
    "        assert np.sum(self.data_ind) > 0, \"The cell is empty!!\"\n",
    "        name_list = ['projmat', 'cyclic', 'full']\n",
    "        \n",
    "        method = self.proj_design['name']\n",
    "        \n",
    "        assert method in name_list, 'No such rule implemented in the current tree class!'\n",
    "        \n",
    "        if method == 'projmat':\n",
    "            \n",
    "            return comp_projmat(self.data[self.data_ind,:], **self.proj_design['params'])\n",
    "        \n",
    "        elif method == 'cyclic':\n",
    "            # cycle through features using height information\n",
    "            # here A is 1D\n",
    "            n_features = self.data.shape[1]\n",
    "            A = np.zeros(n_features)\n",
    "            A[self.height_ % n_features] = 1\n",
    "            return A\n",
    "        else:\n",
    "            # no compression, 'full'\n",
    "            return np.eye(self.data.shape[1])\n",
    "        \n",
    "    def split_rule_function(self, A):\n",
    "        \"\"\"\n",
    "        Given a projection matrix\n",
    "        Returns the best split direction and threshold\n",
    "        Warning: Should only be executed if self.data_ind has at least ONE nonzero element\n",
    "        \"\"\"\n",
    "        assert np.sum(self.data_ind) > 0, \"The cell is empty!\"\n",
    "        name_list = ['cart', 'median', 'median_perturb', '2means']\n",
    "        \n",
    "        method = self.split_design['name']\n",
    "        assert method in name_list, 'No such split rule implemented in current tree class!'\n",
    "        \n",
    "        if 'params' in self.split_design:\n",
    "            params = self.split_design['params']\n",
    "        else:\n",
    "            params = dict()\n",
    "        \n",
    "        if method == 'cart':\n",
    "            return cart_split(self.data[self.data_ind,:], A, self.labels[self.data_ind], **params)\n",
    "        elif method == 'median':\n",
    "            return median_split(self.data[self.data_ind,:], A, **params)\n",
    "        elif method == 'median_perturb':\n",
    "            \n",
    "            node_height = self.height_ # height of this node relative to root of the flex tree\n",
    "            return median_perturb_split(self.data[self.data_ind,:], A, node_height, **params)\n",
    "        else:\n",
    "            return two_means_split(self.data[self.data_ind,:], A, **params)\n",
    "        \n",
    "    \n",
    "    def buildtree(self):\n",
    "        \"\"\"\n",
    "        Recursively build a tree starting from current node as root\n",
    "        Constructs w (projection direction) and threshold for each node\n",
    "        \n",
    "        To execute buildtree, self.data_ind must have at least ONE non-zero entry\n",
    "        \"\"\"\n",
    "        if self.split_design['name'] == 'cart':\n",
    "            assert self.labels is not None, \"You must provide data labels to execute CART!\"\n",
    "        if not self.isLeaf:\n",
    "            ## set projection/transformation/selection matrix\n",
    "            A = self.proj_rule_function()  # A can be dense or sparse matrix\n",
    "    \n",
    "            ## find the best split feature and the best threshold\n",
    "            split_rule = self.split_design['name']\n",
    "            _, self.w_, self.thres_ = self.split_rule_function(A)\n",
    "            \n",
    "            ## transform data to get one or more candidate features\n",
    "            if sps.issparse(self.w_):\n",
    "                projected_data = sps.csr_matrix.dot(self.data[self.data_ind, :], self.w_).squeeze()\n",
    "            else:\n",
    "                projected_data = np.dot(self.data[self.data_ind, :], self.w_) ## project data to 1-D\n",
    "            \n",
    "            data_indices = []\n",
    "            ## data_ind always has the same size as the number of data size\n",
    "            ## data_indices has the same size as the number of data in this cell\n",
    "            for i in range(len(self.data_ind)):\n",
    "                if self.data_ind[i] == 1:\n",
    "                    data_indices.append(i)\n",
    "            assert len(data_indices) == len(projected_data)\n",
    "            data_indices = np.array(data_indices)\n",
    "            \n",
    "            ## split data into left and right\n",
    "            left_indices = projected_data < self.thres_\n",
    "            right_indices = projected_data >= self.thres_\n",
    "            \n",
    "            ## Here, it's still possible that one of the left or right indices is empty array\n",
    "            assert np.sum(left_indices)+np.sum(right_indices) == len(data_indices)\n",
    "            left_ind = data_indices[left_indices]\n",
    "            right_ind = data_indices[right_indices]\n",
    "            ##\n",
    "            n_data = self.data.shape[0]\n",
    "            left = np.zeros(n_data, dtype=bool)\n",
    "            if list(left_ind):\n",
    "                # make assingment only if left_ind is non-empty\n",
    "                left[left_ind] = 1\n",
    "            right = np.zeros(n_data, dtype=bool)\n",
    "            if list(right_ind):\n",
    "                right[right_ind] = 1\n",
    "            \n",
    "            ## build subtrees on left and right partitions\n",
    "            ## By our choice, empty cell will still make a node\n",
    "            self.leftChild_ = flex_binary_trees(self.data, left, self.proj_design, \n",
    "                                                    self.split_design, self.stop_design, \n",
    "                                                    self.height_+1, self.labels)\n",
    "            self.leftChild_.buildtree()\n",
    "            \n",
    "            self.rightChild_ = flex_binary_trees(self.data, right, self.proj_design, \n",
    "                                                     self.split_design, self.stop_design, \n",
    "                                                     self.height_+1, self.labels)\n",
    "            self.rightChild_.buildtree()\n",
    "            \n",
    "        \n",
    "    def train(self):\n",
    "        self.buildtree()\n",
    "        \n",
    "    def predict_one(self, point, predict_type='class'):\n",
    "        return predict_one_bt(self, point, predict_type=predict_type)\n",
    "        \n",
    "    def predict(self, test, predict_type='class'):\n",
    "        return predict_bt(self, test, predict_type=predict_type)\n",
    "            \n",
    "            \n",
    "####-------- Utility functions for binary trees   \n",
    "\n",
    "def retrievalLeaf(btree, query):\n",
    "    \"\"\"\n",
    "    Given a binary partition tree\n",
    "    returns a node that contains query\n",
    "    Here, the cell of a returned leaf node may be empty\n",
    "    \"\"\"\n",
    "    if btree.leftChild_ is None and btree.rightChild_ is None:\n",
    "        return btree\n",
    "    if sps.issparse(btree.w_):\n",
    "        val = sps.csr_matrix.dot(btree.w_.T, query).squeeze()\n",
    "    else:\n",
    "        val = np.dot(btree.w_, query)\n",
    "    if val < btree.thres_:\n",
    "        return retrievalLeaf(btree.leftChild_, query)\n",
    "    else:\n",
    "        return retrievalLeaf(btree.rightChild_, query)\n",
    "    \n",
    "def retrievalSet(btree, query):\n",
    "    \"\"\"\n",
    "    Given a binary partition tree\n",
    "    returns indices of points in the cell that contains query point\n",
    "    By design, this set of indices will NOT be all zeros\n",
    "    \"\"\"\n",
    "    ## base case: return data indices if leaf node is reached    \n",
    "    if btree.leftChild_ is None and btree.rightChild_ is None:\n",
    "        ## By our recursion, data_ind returned won't be all zeros\n",
    "        assert np.sum(data_ind) != 0, \"Something is wrong!\"\n",
    "        return btree.data_ind\n",
    "        \n",
    "    ## check which subset does the query belong to\n",
    "    if sps.issparse(btree.w_):\n",
    "        val = sps.csr_matrix.dot(btree.w_.T, query)\n",
    "    else:\n",
    "        val = np.dot(btree.w_, query)\n",
    "        \n",
    "    if val < btree.thres_: \n",
    "        if (btree.leftChild_ is None) or (np.sum(btree.leftChild_.data_ind)==0):\n",
    "            # use parent cell as the retrieval set \n",
    "            # parent cell is guaranteed to be non-empty\n",
    "            return btree.data_ind\n",
    "        return retrievalSet(btree.leftChild_, query)\n",
    "    else:\n",
    "        if (btree.rightChild_ is None) or (np.sum(btree.rightChild_.data_ind)==0):\n",
    "            return btree.data_ind\n",
    "        return retrievalSet(btree.rightChild_, query)\n",
    "        \n",
    "def getDepth(btree, depth):\n",
    "    \"\"\"\n",
    "    find out the depth (maximal height of all branch) of a binary tree\n",
    "    depth is the depth of current node\n",
    "    \"\"\"\n",
    "    ## via DFS\n",
    "    if btree.leftChild_ is None and btree.rightChild_ is None:\n",
    "        return depth\n",
    "        \n",
    "    depthL = getDepth(btree.leftChild_, depth+1)\n",
    "    depthR = getDepth(btree.rightChild_, depth+1)\n",
    "        \n",
    "    if depthL >= depthR:\n",
    "        return depthL\n",
    "    else:\n",
    "        return depthR\n",
    "        \n",
    "def predict_one_bt(btree, point, predict_type='class'):\n",
    "    assert list(btree.labels), \"This tree has no associated data labels!\"\n",
    "    set_ind = retrievalSet(btree, point) \n",
    "    \n",
    "    if predict_type == 'class':\n",
    "        if np.sum(set_ind)==0:\n",
    "            # using retrievalSet makes sure set_ind is not all zeros\n",
    "            # but this check is just in case\n",
    "            return round(np.mean(btree.labels))\n",
    "        return round(np.mean(btree.labels[set_ind]))\n",
    "        \n",
    "    else:\n",
    "        # regression\n",
    "        if np.sum(set_ind)==0:\n",
    "            return np.mean(btree.labels)\n",
    "        return np.mean(btree.labels[set_ind])\n",
    "        \n",
    "    \n",
    "def predict_bt(btree, test, predict_type='class'):\n",
    "    \"\"\"\n",
    "    Returns a list of predictions corresponding to test set\n",
    "    \"\"\"\n",
    "    predictions = list()\n",
    "    for point in test:\n",
    "        predictions.append(predict_one_bt(btree, point, predict_type=predict_type))\n",
    "    return predictions\n",
    "\n",
    "        \n",
    "def k_nearest(tree, query):\n",
    "    \"\"\"\n",
    "    Given dataset organized in a tree structure  ----- TO BE IMPLEMENTED\n",
    "    find the approximate k nearest neighbors of query point\n",
    "    \"\"\"\n",
    "    return k_nearest\n",
    "        \n",
    "    \n",
    "###\n",
    "def printPartition(tree, level):\n",
    "    \"\"\"\n",
    "    Starting from root of the tree, traverse each node at given level\n",
    "    and print the partitioning it holds\n",
    "    Can be used for testing purposes\n",
    "    \"\"\"\n",
    "    if tree.height_ == level or (tree.leftChild_ is None and tree.rightChild_ is None):\n",
    "        ind_set = []\n",
    "        for i in range(len(tree.data_ind)):\n",
    "            if tree.data_ind[i] == 1:\n",
    "                ind_set.append(i)\n",
    "        print(ind_set)\n",
    "    else:\n",
    "        printPartition(tree.leftChild_, level)\n",
    "        printPartition(tree.rightChild_, level)\n",
    "        \n",
    "def traverseLeaves(tree):\n",
    "    \"\"\"\n",
    "    This traversal works for both balanced and unbalanced binary trees\n",
    "    \"\"\"\n",
    "    if tree.leftChild_ is None and tree.rightChild_ is None:\n",
    "        # leaf node\n",
    "        yield tree\n",
    "    \n",
    "    if tree.leftChild_ is not None:\n",
    "        for t in traverseLeaves(tree.leftChild_):\n",
    "            yield t\n",
    "    if tree.rightChild_ is not None:\n",
    "        for t in traverseLeaves(tree.rightChild_):\n",
    "            yield t\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######------- A class of master trees inspired by Kpotufe's adaptive tree structure\n",
    "## these are not binary trees but are \"meta-trees\" built on binary trees\n",
    "## it is used to prune a binary tree as on-the-fly\n",
    "\n",
    "class master_trees(object):\n",
    "    \n",
    "    def __init__(self, data, labels=None, parent_slave_tree=None, child_slave_tree_params=None, \n",
    "                 curr_height=0, max_height=10):\n",
    "        \"\"\"\n",
    "        Each instance of this class has two links: a link to a parent slave tree and a link to\n",
    "        a child slave tree; each slave tree is an instance of the flex_binary_tree class\n",
    "        Exceptions:\n",
    "            -Leaf nodes only have parent slave trees\n",
    "            -Root node only has a child slave tree\n",
    "        \n",
    "        data: data of the entire dataset\n",
    "        (unlike flex_binary_tree, each node of the master tree doesn't have data_indices as attribute; it is because\n",
    "        this information is already contained in its slave tree;\n",
    "        in fact, there is no need to use \"data\" as attribute either?)\n",
    "        parent_slave_tree: slave_tree that leads to the creation of this master_tree \n",
    "            - if None, master tree is root \n",
    "            - the parent slave tree also has a pointer to this master tree\n",
    "        child_slave_tree_params: a dict of parameters of the slave tree if default (RP-tree) is not used\n",
    "            - if not None, it should has keys \"proj_design\", \"split_design\", \"stop_design\"\n",
    "        child_slave_tree: slave tree that is constructed by this master tree, by calling grow_child\n",
    "            - the child slave tree also has a pointer to this master tree(is this necessary?)\n",
    "        curr_height: absolute level of this master tree node, counting internal levels in slave trees\n",
    "        max_height: maximal level that is allowed to be reached by the entire tree \n",
    "            - used independent of the slave tree stopping test\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.slave_tree_params=child_slave_tree_params\n",
    "        self.n_rep = 0\n",
    "        if self.slave_tree_params is not None and 'repeat' in self.slave_tree_params:\n",
    "            self.n_rep = self.slave_tree_params['repeat']\n",
    "        self.curr_height = curr_height\n",
    "        self.max_height = max_height\n",
    "        self.leaves_list = list()\n",
    "        self.parent_slave_tree = None\n",
    "        \n",
    "        ## initialize parent slave tree if exists\n",
    "        if parent_slave_tree is not None:\n",
    "            self.parent_slave_tree = parent_slave_tree\n",
    "            self.parent_slave_tree.master_tree = self\n",
    "        ## for testing purpose\n",
    "        print_diam_leaves(self)\n",
    "        \n",
    "    def grow_child(self):\n",
    "        \"\"\"\n",
    "        Method to grow a child slave tree from root to leaves\n",
    "        \"\"\" \n",
    "        if self.parent_slave_tree is None and self.slave_tree_params is None:\n",
    "            # default parameters\n",
    "            data_ind = np.ones(self.data.shape[0], dtype=bool)\n",
    "            proj_design = {'name':'projmat', 'params':{'name':'dasgupta'}}\n",
    "                \n",
    "            ## Here, data diameter is the diameter of the entire dataset (since this is the root of master tree)\n",
    "            ddiameter,_,_ = data_diameter(self.data) \n",
    "            #print(self.data.shape)\n",
    "            split_design = {'name':'median_perturb', 'params':{'diameter':ddiameter}}\n",
    "            stop_design = {'name':'cell_size'}\n",
    "            stop_design['params']={'diameter':ddiameter}\n",
    "            #print(\"pass\")\n",
    "            \n",
    "        elif self.parent_slave_tree is None and self.slave_tree_params is not None:\n",
    "            # user-defined child_slave_tree params\n",
    "            data_ind = np.ones(self.data.shape[0], dtype=bool)\n",
    "            proj_design = self.slave_tree_params['proj_design']\n",
    "            split_design = self.slave_tree_params['split_design']\n",
    "            stop_design = self.slave_tree_params['stop_design']\n",
    "        elif self.parent_slave_tree is not None:\n",
    "            # this will be invoked as long as this master tree is not a root\n",
    "            # if a parent slave tree exists, \n",
    "            # we always use its params to define the new child slave tree\n",
    "            data_ind = self.parent_slave_tree.data_ind\n",
    "            proj_design = self.parent_slave_tree.proj_design\n",
    "            stop_design = self.parent_slave_tree.stop_design\n",
    "            #print(\"Current diameter stored is %f\" %stop_design['params']['diameter'])\n",
    "            split_design = self.parent_slave_tree.split_design\n",
    "            stop_design = self.parent_slave_tree.stop_design\n",
    "        else:\n",
    "            print(\"Something is wrong\")\n",
    "        \n",
    "        ## create child slave tree\n",
    "        # By design, we use the relative height in child slave tree\n",
    "        self.child_slave_tree = flex_binary_trees(self.data, data_indices=data_ind, proj_design=proj_design,\n",
    "                                        split_design=split_design, stop_design=stop_design,\n",
    "                                        height=0, labels=self.labels, master_tree=self)\n",
    "        ### employs child slave tree      \n",
    "        self.child_slave_tree.buildtree()\n",
    "        depth = getDepth(self.child_slave_tree, 0) #get relative depth of the child slave tree\n",
    "        print(\"The relative depth of the new child tree is %d\" %depth)\n",
    "        for i in range(self.n_rep):\n",
    "            ## select tree with shortest depth\n",
    "            slave_tree = flex_binary_trees(self.data, data_ind, proj_design=proj_design, split_design=split_design, \n",
    "                              stop_design=stop_design, height=0, labels=self.labels, master_tree=self)\n",
    "            slave_tree.buildtree()\n",
    "            if depth > getDepth(slave_tree, 0):\n",
    "                self.child_slave_tree = slave_tree\n",
    "        \n",
    "        \n",
    "    def iter_child_leaves(self):\n",
    "        return traverseLeaves(self.child_slave_tree)\n",
    "        \n",
    "    def build_master_trees(self):\n",
    "        self.grow_child()\n",
    "        leaves_gen = self.iter_child_leaves() # up to this point, nodes are binary trees\n",
    "        \n",
    "        for leaf in leaves_gen:\n",
    "            ## by design, curr_height is the absolute height of this master node (counting slave tree levels)\n",
    "            # Update the diameter of data in leaf\n",
    "            ddiam,_,_ = data_diameter(self.data[leaf.data_ind])\n",
    "            if 'params' in leaf.split_design and 'diameter' in leaf.split_design['params']:\n",
    "                leaf.split_design['params']['diameter'] = ddiam\n",
    "            if 'params' in leaf.stop_design and 'diameter' in leaf.stop_design['params']:\n",
    "                leaf.stop_design['params']['diameter'] = ddiam\n",
    "            \n",
    "            # child tree leaves will become parent slave trees for the new master tree\n",
    "            new_master_tree = master_trees(self.data, labels=self.labels, parent_slave_tree=leaf, \n",
    "                                            curr_height=leaf.height_+self.curr_height ,max_height=self.max_height )\n",
    "            #display(new_master_tree.parent_slave_tree)\n",
    "            self.leaves_list.append(new_master_tree)\n",
    "            \n",
    "        ## Decide whether to grow the next level of master trees\n",
    "        if not mtree_test_stop(self.leaves_list, self.max_height):\n",
    "            for mleaf in self.leaves_list:\n",
    "                if np.sum(mleaf.parent_slave_tree.data_ind) > 1:\n",
    "                    ## if max height is not achieved in any cell of slave tree\n",
    "                    # and that the cell is not empty, continue grow it\n",
    "                    print(\"A new master tree built!\")\n",
    "                    mleaf.build_master_trees()\n",
    "                \n",
    "    def train(self):\n",
    "        ## interface with cross-validation evalutaion\n",
    "        self.build_master_trees()\n",
    "        \n",
    "    def predict_one(self, point, predict_type='class'):\n",
    "        return predict_one_mt(self, point, predict_type=predict_type)\n",
    "     \n",
    "    def predict(self, test, predict_type='class'):\n",
    "        ## interface with cross-validation evaluation\n",
    "        return predict_mt(self, test, predict_type=predict_type)\n",
    "\n",
    "            \n",
    "\n",
    "####----------- Utility functions for master trees\n",
    "def mtree_test_stop(mtree_leaves_list, max_height):\n",
    "    count=1\n",
    "    for mtree_leaf in mtree_leaves_list:\n",
    "        if mtree_leaf.curr_height >= max_height:\n",
    "            #print(\"Stop test is passed at height %d\" %mtree_leaf.curr_height)\n",
    "            return True\n",
    "        print(\"Depth of the %d-th leaf is %d\" %(count,mtree_leaf.curr_height))\n",
    "        data_ind = mtree_leaf.parent_slave_tree.data_ind\n",
    "        ddiam,_,_ = data_diameter(mtree_leaf.data[data_ind,:])\n",
    "        #print(\"Diameter of the %d-th leaf is %f\" %(count, ddiam))\n",
    "        count+=1\n",
    "    \n",
    "    return False\n",
    "       \n",
    "def retrievalLeaf_mtree(mtree, query):\n",
    "    ## base case\n",
    "    if not mtree.leaves_list:\n",
    "        return mtree\n",
    "    \n",
    "    ##\n",
    "    #find the leaf containing query using its binary slave tree\n",
    "    slave_leaf = retrievalLeaf(mtree.child_slave_tree, query) \n",
    "    return retrievalLeaf_mtree(slave_leaf.master_tree, query) #recurse on master tree of the found leaf\n",
    "\n",
    "def traverseLeaves_mtree(mtree):\n",
    "    if not mtree.leaves_list:\n",
    "        yield mtree\n",
    "    \n",
    "    for leaf in mtree.iter_child_leaves():\n",
    "        traverseLeaves_mtree(leaf.master_tree)\n",
    "\n",
    "\n",
    "def print_mtree_leaves(mtree):\n",
    "    count = 0\n",
    "    if not mtree.leaves_list:\n",
    "        if mtree.parent_slave_tree is not None:\n",
    "            indices = np.arange(mtree.data.shape[0])\n",
    "            display(indices[mtree.parent_slave_tree.data_ind])\n",
    "            #return np.sum(mtree.parent_slave_tree.data_ind)\n",
    "            return count+1\n",
    "        else:\n",
    "            print(\"Root node\")\n",
    "            \n",
    "    else:\n",
    "        for leaf in mtree.leaves_list:\n",
    "            count += print_mtree_leaves(leaf)\n",
    "        return count\n",
    "\n",
    "def apply_mtree_leaves(mtree, myfunc):\n",
    "    if not mtree.leaves_list:\n",
    "        if mtree.parent_slave_tree is not None:\n",
    "            myfunc(mtree.parent_slave_tree)\n",
    "        else:\n",
    "            print(\"Root node\")\n",
    "    else:\n",
    "        for leaf in mtree.leaves_list:\n",
    "            apply_mtree_leaves(leaf)\n",
    "\n",
    "def print_diam_leaves(mtree):\n",
    "    def myfunc(f_tree):\n",
    "        ddiam,_,_ = data_diameter(f_tree.data[f_tree.data_ind])\n",
    "        print(ddiam)\n",
    "    apply_mtree_leaves(mtree, myfunc)\n",
    "    \n",
    "    \n",
    "def predict_one_mt(mtree, point, predict_type='class'):\n",
    "    \"\"\"\n",
    "    In the future, change retrieval method to retrievalSet_mtree, which should have \n",
    "    better performance\n",
    "    \"\"\"\n",
    "    mLeaf = retrievalLeaf_mtree(mtree, point)\n",
    "    set_ind = mLeaf.parent_slave_tree.data_ind\n",
    "    \n",
    "    if predict_type == 'class':\n",
    "        #print(round(np.mean(mcell.slave_tree.labels[set_ind])))\n",
    "        if np.sum(set_ind)==0:\n",
    "            # if set index is an empty list\n",
    "            return round(np.mean(mLeaf.parent_slave_tree.labels))\n",
    "        return round(np.mean(mLeaf.parent_slave_tree.labels[set_ind]))\n",
    "        \n",
    "    else:\n",
    "        # regression\n",
    "        if np.sum(set_ind)==0:\n",
    "            return np.mean(mLeaf.parent_slave_tree.labels)\n",
    "        return np.mean(mLeaf.parent_slave_tree.labels[set_ind])\n",
    "    \n",
    "def predict_mt(mtree, test, predict_type='class'):\n",
    "    \"\"\"\n",
    "    Returns a list of predictions corresponding to test set\n",
    "    \"\"\"\n",
    "    predictions = list()\n",
    "    for point in test:\n",
    "        predictions.append(predict_one_mt(mtree, point, predict_type=predict_type))\n",
    "    return predictions        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class forest(object):\n",
    "    def __init__(self, data, labels=None, tree_design=None, predictor_type='regress',\n",
    "                  n_trees=10, n_samples=100, n_features=None):\n",
    "        \"\"\"\n",
    "        tree_design: A dictionary containing \n",
    "          - tree: specifies type of the tree; flex or master\n",
    "          - a proj_design dictionary, \n",
    "          - a split_design dictionary,\n",
    "          - a stop_design dictionary\n",
    "        \n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tree_design = tree_design\n",
    "        self.predictor_type = predictor_type\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = list() # store trees for re-use\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        \n",
    "    def reset_sample_size(self, n_samples=None, n_features=None):\n",
    "        if n_samples is not None:\n",
    "            self.n_samples = n_samples\n",
    "        if n_features is not None:\n",
    "            self.n_features = n_features\n",
    "            \n",
    "    def reset_predictor_type(self, method):\n",
    "        self.predictor_type = method\n",
    "        \n",
    "    def build_forest_classic(self, isPredict=True):\n",
    "        if isPredict:\n",
    "            assert labels is not None, \"Data labels missing!\"\n",
    "        tree_type = self.tree_design['tree']\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            # sample data points with replacement\n",
    "            # note numpy indexing supports repetitive/duplicate indexing\n",
    "            data_ind = np.random.choice(self.data.shape[0], self.n_samples, replace=True)\n",
    "            data_tree = self.data[data_ind,:] # data unique to this tree\n",
    "            \n",
    "            if self.n_features is not None:\n",
    "                ## optionally subsample features (note: this is NOT done in the original RF)\n",
    "                feature_ind = np.random.choice(self.data.shape[1], self.n_features, replace=True)\n",
    "                data_tree = data_tree[:,feature_ind] # features unique to this tree\n",
    "            \n",
    "            if isPredict:\n",
    "                labels_tree = self.labels[data_ind]\n",
    "            else:\n",
    "                labels_tree = None\n",
    "            \n",
    "            if tree_type == 'flex':\n",
    "                proj_design, split_design, stop_design = self.tree_design['proj_design'],\\\n",
    "                   self.tree_design['split_design'],self.tree_design['stop_design']\n",
    "                    \n",
    "                self.trees.append(flex_binary_trees(data_tree, np.ones(data_tree.shape[0], dtype=bool), \n",
    "                                        proj_design,split_design, stop_design, labels=labels_tree))\n",
    "            else:\n",
    "                ## adaptive RP tree\n",
    "                if self.tree_design is not None:\n",
    "                    # user-defined adaptive tree\n",
    "                    self.trees.append(master_trees(data_tree, labels=labels_tree, \n",
    "                                                   child_slave_tree_params = self.tree_design))\n",
    "                else:\n",
    "                    # default use Kpotufe's adaptive tree\n",
    "                    self.trees.append(master_trees(data_tree, labels=labels_tree))\n",
    "    \n",
    "    \n",
    "    def build_forest_with_tree_preproc(self, method):\n",
    "        pass\n",
    "        \n",
    "    def build_forest_with_forest_preproc(self, method):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        self.build_forest_classic(isPredict=True)\n",
    "    \n",
    "    def predict_one(self, point):\n",
    "        \"\"\"\n",
    "        Predictor_type can be either 'class' for classification,\n",
    "        'regress' for regression, or a user-defined callable function\n",
    "        \"\"\"\n",
    "        assert self.trees, \"You must first build a forest\"\n",
    "        \n",
    "        if self.predictor_type == 'class':\n",
    "            ## binary classification\n",
    "            avg_predict = 0\n",
    "            for tree in self.trees:\n",
    "                avg_predict += tree.predict_one(point, predict_type='class')\n",
    "            if avg_predict/float(len(self.trees)) > 0.5:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        elif self.predictor_type == 'regress':\n",
    "            ## regression\n",
    "            avg_predict = 0\n",
    "            for tree in self.trees:\n",
    "                avg_predict += tree.predict_one(point, predict_type='regress')\n",
    "            return avg_predict/float(len(self.trees))\n",
    "        else:\n",
    "            print(\"Unrecognized prediction method!\")\n",
    "           \n",
    "    def predict(self, test):\n",
    "        predictions = list()\n",
    "        for point in test:\n",
    "            predictions.append(self.predict_one(point))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Utility functions for sampling and evaluation\n",
    "\n",
    "def subsample(data, n_samples, n_features=None):\n",
    "    \"\"\"\n",
    "    sample WITH replacement\n",
    "    \"\"\"\n",
    "    ind_data = np.random.choice(data.shape[0], size=n_samples, replace=True)\n",
    "    if n_features is not None:\n",
    "        ind_features = np.random.choice(data.shape[1], size=n_features, replace=True)\n",
    "        return data[ind_data, ind_features]\n",
    "        \n",
    "    return data[ind_data,:] \n",
    "\n",
    "def cross_valid_split(n_data, n_folds):\n",
    "    \"\"\"\n",
    "    Given size of the data and number of folds\n",
    "    Returns n_folds disjoint sets of indices, where indices\n",
    "    in each fold are chosen u.a.r. without replacement\n",
    "    \"\"\"\n",
    "    data_ind = list(range(n_data))    \n",
    "    folds = list()\n",
    "    fold_size = floor(n_data/n_folds)\n",
    "    for i in range(n_folds):\n",
    "        if i < n_folds-1:\n",
    "            fold = list()\n",
    "            while len(fold) <= fold_size:\n",
    "                index = random.randrange(len(data_ind))\n",
    "                fold.append(data_ind.pop(index))\n",
    "            folds.append(fold)\n",
    "        else:\n",
    "            ## assign all remaining data to the last fold\n",
    "            folds.append(data_ind)\n",
    "    return folds\n",
    "\n",
    "def zero_one_loss(labels, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == predictions[i]:\n",
    "            correct += 1\n",
    "    loss = (len(labels)-correct)/float(len(labels))\n",
    "    return loss\n",
    "\n",
    "def explained_var_loss(labels, predictions):\n",
    "    #res_var = np.sum(np.array([diff**2 for diff in labels-predictions]))\n",
    "    res_var = np.var(np.array(labels)-np.array(predictions))\n",
    "    tot_var = np.var(np.array(labels))\n",
    "    \n",
    "    return 1-res_var/tot_var\n",
    "\n",
    "def l2_loss(labels, predictions):\n",
    "    loss = np.linalg.norm(np.array(labels)-np.array(predictions))\n",
    "    return loss/(len(labels)**(1/2))\n",
    "\n",
    "def csize_decrease_rate(data, tree):\n",
    "    \"\"\"\n",
    "    This is an unsupervised evaluation, which tries to capture how fast\n",
    "    the data size of a cell decreases after building a tree\n",
    "    \"\"\"\n",
    "    diam_s,_,_ = data_diameter(data)\n",
    "    diam_f = 0\n",
    "    \n",
    "    if hasattr(tree, 'slave_tree'):\n",
    "        ## if this is a master tree\n",
    "        for leaf in traverseLeaves_mtree(tree):\n",
    "            diam, _, _ = data_diameter(leaf.slave_tree.data[leaf.slave_tree.data_ind, :])\n",
    "            if diam > diam_f:\n",
    "                diam_f = diam\n",
    "            \n",
    "    else:\n",
    "        ## if this is normal binary tree\n",
    "        for leaf in traverseLeaves(tree):\n",
    "            diam, _, _ = data_diameter(leaf.data[leaf.data_ind,:])\n",
    "            if diam > diam_f:\n",
    "                diam_f = diam\n",
    "        \n",
    "    return (diam_s/diam_f)/getDepth(tree)\n",
    "        \n",
    "\n",
    "def cross_valid_eval(data, labels, n_folds, loss, algorithm, need_ind=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Given data and labels, a loss function, and a method\n",
    "    generate a list of cv-losses\n",
    "    \n",
    "    \"\"\"\n",
    "    ## generate random folds\n",
    "    folds_ind = cross_valid_split(data.shape[0], n_folds)\n",
    "    losses = list()\n",
    "    for fold_ind in folds_ind:\n",
    "        print(\"Evaluating the %d-th fold\" %(len(losses)+1))\n",
    "        test_ind = fold_ind\n",
    "        folds_ind_ = list(folds_ind) # this ensures we are not modifying the original list!\n",
    "        folds_ind_.remove(fold_ind)\n",
    "        train_ind = [item for sublist in folds_ind_ for item in sublist] #flatten remaining index set\n",
    "        ## Further divide the data into train and test\n",
    "        data_tr = data[train_ind,:]\n",
    "        labels_tr = labels[train_ind]\n",
    "        data_tt = data[test_ind,:]\n",
    "        labels_tt = labels[test_ind]\n",
    "        # train the algorithm \n",
    "        if need_ind:\n",
    "            data_ind = np.ones(data_tr.shape[0], dtype=bool)\n",
    "            alg = algorithm(data_tr, data_indices=data_ind, labels=labels_tr, **kwargs) #init\n",
    "        else:\n",
    "            # RF and master trees don't need to be given an index \n",
    "            alg = algorithm(data_tr, labels=labels_tr, **kwargs) #init\n",
    "        alg.train()\n",
    "        #c = print_mtree_leaves(alg) ## added should be removed\n",
    "        #print(\"There are %d partitions\" %c)\n",
    "        \n",
    "        # calculate loss on the current fold\n",
    "        losses.append(loss(labels_tt, alg.predict(data_tt)))\n",
    "        #print(labels[0],alg.predict(data_tt)[0])\n",
    "        del alg\n",
    "    return losses                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######--- Utility functions for data preprocessing\n",
    "\n",
    "# get data from url link and store\n",
    "def download_data(fname, url_name, force = False):\n",
    "    if force or not os.path.exists(fname):\n",
    "        print('Downloading data from the internet...')\n",
    "        try:\n",
    "            urllib.urlretrieve(url_name, fname)\n",
    "        except Exception as e:\n",
    "            print(\"Unable to retrieve file from given url\")\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "        \n",
    "\n",
    "# pickle or get pickled data with desired dataname\n",
    "def maybe_pickle(dataname, data = None, force = False, verbose = True):\n",
    "    \"\"\"\n",
    "    Process and pickle a dataset if not present\n",
    "    \"\"\"\n",
    "    filename = dataname + '.pickle'\n",
    "    if force or not os.path.exists(filename):\n",
    "        # pickle the dataset\n",
    "        print('Pickling data to file %s' % filename)\n",
    "        try:\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save to', filename, ':', e) \n",
    "    else:\n",
    "        print('%s already present - Skipping pickling.' % filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "## Download sonar dataset for classification problems\n",
    "fname = \"sonar.all_data.csv\"\n",
    "url_name = \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "download_data(fname, url_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data has shape 208 by 60\n",
      "Preprocessed labels has length 208\n",
      "sonar.all_data.pickle already present - Skipping pickling.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.0151,  0.032 ,  0.0599, ...,  0.0019,  0.0023,  0.0062],\n",
       "        [ 0.0336,  0.0294,  0.0476, ...,  0.0015,  0.0069,  0.0051],\n",
       "        [ 0.0195,  0.0213,  0.0058, ...,  0.0095,  0.0021,  0.0053],\n",
       "        ..., \n",
       "        [ 0.0519,  0.0548,  0.0842, ...,  0.0047,  0.0048,  0.0053],\n",
       "        [ 0.1371,  0.1226,  0.1385, ...,  0.0079,  0.0146,  0.0051],\n",
       "        [ 0.0089,  0.0274,  0.0248, ...,  0.0069,  0.006 ,  0.0018]]),\n",
       " 'labels': array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,\n",
       "         1.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "         1.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
       "         1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
       "         0.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,\n",
       "         0.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,\n",
       "         1.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,\n",
       "         1.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "         1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,\n",
       "         1.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,\n",
       "         1.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### module for preprocessing sonar data and pickling it\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    " \n",
    "# Convert string column to float\n",
    "# this is for data conversion\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string\n",
    "# this is for label conversion\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "\n",
    "### load and preprocess data\n",
    "fname = 'sonar.all_data.csv'\n",
    "data = load_csv(fname)\n",
    "for i in range(len(data[0])-1):\n",
    "    str_column_to_float(data, i)\n",
    "    \n",
    "lookup = str_column_to_int(data, -1)\n",
    "data = np.array(data)\n",
    "## random shuffling\n",
    "pInd = np.random.permutation(data.shape[0])\n",
    "data = data[pInd]\n",
    "X = data[:,:data.shape[-1]-1]\n",
    "labels = data[:, -1]\n",
    "print(\"Preprocessed data has shape %d by %d\" % X.shape)\n",
    "print(\"Preprocessed labels has length %d\" % len(labels))\n",
    "## pickle data\n",
    "maybe_pickle('sonar.all_data', {'data':X, 'labels':labels}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sonar.all_data.pickle already present - Skipping pickling.\n",
      "number of one's in labels are 111\n"
     ]
    }
   ],
   "source": [
    "data_dict = maybe_pickle('sonar.all_data')\n",
    "data = data_dict['data']\n",
    "labels = data_dict['labels']\n",
    "data_tr = data[:104,:]\n",
    "labels_tr = labels[:104]\n",
    "data_tt = data[104:,:]\n",
    "labels_tt = labels[104:]\n",
    "print(\"number of one's in labels are %d\" %labels.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Test: CART-decision tree + Breiman's features selection/projection rule\n",
    "sparsity = [1, 3, 10]\n",
    "target_dim = [1, 5, 10]\n",
    "stop_rule = ['naive', 'cell_size']\n",
    "scores_c1 = list()\n",
    "params_c1 = list()\n",
    "\n",
    "##\n",
    "for s in sparsity:\n",
    "    for t_dim in target_dim:\n",
    "        for s_rule in stop_rule:\n",
    "            proj_design={'name':'projmat','params':{'name':'breiman','sparsity':s,'target_dim':t_dim}}\n",
    "            split_design={'name':'cart'} \n",
    "            stop_design={'name':s_rule}\n",
    "            if s_rule == 'cell_size':\n",
    "                ddiam, _, _ = data_diameter(data_tr)\n",
    "                stop_design['params'] = {'diameter':ddiam, 'max_level':2*np.log(data_tr.shape[0])}\n",
    "\n",
    "            kwargs = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "            scores_c1.append(cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, flex_binary_trees, \n",
    "                                              need_ind=True, **kwargs))\n",
    "            params_c1.append([s, t_dim, s_rule])\n",
    "            \n",
    "#scores = cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, flex_binary_trees)\n",
    "#score1 = np.mean(scores)\n",
    "\n",
    "## best param: 3, 10, cell_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_dict = {'sparsity':list(), 'target_dim':list(), 'stop_rule': list(), 'zero-one-loss':list()}\n",
    "count = 0\n",
    "for s in sparsity:\n",
    "    for t_dim in target_dim:\n",
    "        for s_rule in stop_rule:\n",
    "            s_dict['sparsity'].append(s)\n",
    "            s_dict['target_dim'].append(t_dim)\n",
    "            s_dict['stop_rule'].append(s_rule)\n",
    "            s_dict['zero-one-loss'].append(np.mean(scores_c1[count]))\n",
    "            count += 1  \n",
    "s_dict_c1 = s_dict\n",
    "df = DataFrame(s_dict_c1)\n",
    "\n",
    "print('CART-DCTree-Breiman')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Test: CART-decision tree + RP-dense projection (dasgupta)\n",
    "target_dim = [1, 5, 10]\n",
    "stop_rule = ['naive', 'cell_size']\n",
    "scores_c2 = list()\n",
    "params_c2 = list()\n",
    "##\n",
    "for t_dim in target_dim:\n",
    "    for s_rule in stop_rule:\n",
    "        proj_design = {'name':'projmat','params':{'name':'dasgupta','target_dim':t_dim}}\n",
    "        split_design = {'name':'cart'}\n",
    "        stop_design = {'name':s_rule}\n",
    "        if s_rule == 'cell_size':\n",
    "                ddiam, _, _ = data_diameter(data_tr)\n",
    "                stop_design['params'] = {'diameter':ddiam, 'max_level':2*np.log(data_tr.shape[0])}\n",
    "        kwargs = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "        scores_c2.append(cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, flex_binary_trees, \n",
    "                                          need_ind=True, **kwargs))\n",
    "        params_c2.append([t_dim, s_rule])\n",
    "\n",
    "## best param: 10, cell_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_dict = {'target_dim':list(), 'stop_rule': list(), 'zero-one-loss':list()}\n",
    "count = 0\n",
    "\n",
    "for t_dim in target_dim:\n",
    "    for s_rule in stop_rule:\n",
    "        s_dict['target_dim'].append(t_dim)\n",
    "        s_dict['stop_rule'].append(s_rule)\n",
    "        s_dict['zero-one-loss'].append(np.mean(scores_c2[count]))\n",
    "        count += 1  \n",
    "s_dict_c2 = s_dict\n",
    "df = DataFrame(s_dict_c2)\n",
    "\n",
    "print('CART-DCTree-RPdense')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Test: CART-decision tree + RP-sparse projection (randomer forest paper)\n",
    "target_dim = [1, 5, 10]\n",
    "stop_rule = ['naive', 'cell_size']\n",
    "scores_c3 = list()\n",
    "params_c3 = list()\n",
    "##\n",
    "for t_dim in target_dim:\n",
    "    for s_rule in stop_rule:\n",
    "        proj_design = {'name':'projmat','params':{'name':'tomita','target_dim':t_dim}}\n",
    "        split_design = {'name':'cart'}\n",
    "        stop_design = {'name':s_rule}\n",
    "        if s_rule == 'cell_size':\n",
    "                ddiam, _, _ = data_diameter(data_tr)\n",
    "                stop_design['params'] = {'diameter':ddiam, 'max_level':2*np.log(data_tr.shape[0])}\n",
    "        kwargs = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "        scores_c3.append(cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, flex_binary_trees, \n",
    "                                          need_ind=True, **kwargs))\n",
    "        params_c3.append([t_dim, s_rule])\n",
    "\n",
    "## best param: 1, cell_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_dict = {'target_dim':list(), 'stop_rule': list(), 'zero-one-loss':list()}\n",
    "count = 0\n",
    "\n",
    "for t_dim in target_dim:\n",
    "    for s_rule in stop_rule:\n",
    "        s_dict['target_dim'].append(t_dim)\n",
    "        s_dict['stop_rule'].append(s_rule)\n",
    "        s_dict['zero-one-loss'].append(np.mean(scores_c3[count]))\n",
    "        count += 1  \n",
    "s_dict_c3 = s_dict\n",
    "df = DataFrame(s_dict_c3)\n",
    "\n",
    "print('CART-DCTree-RPsparse')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test: Median split tree + Breiman's feature selection/projection rule\n",
    "sparsity = [1, 3, 10]\n",
    "target_dim = [1, 5, 10]\n",
    "stop_rule = ['naive', 'cell_size']\n",
    "perturb = [True, False]\n",
    "scores_m1 = list()\n",
    "params_m1 = list()\n",
    "\n",
    "##\n",
    "for s in sparsity:\n",
    "    for t_dim in target_dim:\n",
    "        for s_rule in stop_rule:\n",
    "            for p in perturb:\n",
    "                proj_design={'name':'projmat','params':{'name':'breiman','sparsity':s,'target_dim':t_dim}}\n",
    "                if p:\n",
    "                    ddiam,_,_ = data_diameter(data_tr)\n",
    "                    split_design={'name':'median_perturb', 'params':{'diameter':ddiam, 'root_height':0}}\n",
    "                else:\n",
    "                    split_design = {'name':'median'}\n",
    "                stop_design={'name':s_rule}\n",
    "                if s_rule == 'cell_size':\n",
    "                    ddiam, _, _ = data_diameter(data_tr)\n",
    "                    stop_design['params'] = {'diameter':ddiam, 'max_level':2*np.log(data_tr.shape[0])}\n",
    "                kwargs = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "                scores_m1.append(cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, flex_binary_trees,\n",
    "                                                  need_ind=True, **kwargs))\n",
    "                params_m1.append([s, t_dim, s_rule, p])\n",
    "\n",
    "## best param: 10,10,cell_size,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparsity = [1, 3, 10]\n",
    "target_dim = [1, 5, 10]\n",
    "stop_rule = ['naive', 'cell_size']\n",
    "perturb = [True, False]\n",
    "##\n",
    "s_dict = {'sparsity':list(), 'target_dim':list(), 'stop_rule': list(), 'perturb':list(), 'zero-one-loss':list()}\n",
    "count = 0\n",
    "for s in sparsity:\n",
    "    for t_dim in target_dim:\n",
    "        for s_rule in stop_rule:\n",
    "            for p in perturb:\n",
    "                s_dict['sparsity'].append(s)\n",
    "                s_dict['target_dim'].append(t_dim)\n",
    "                s_dict['stop_rule'].append(s_rule)\n",
    "                s_dict['perturb'].append(str(p))\n",
    "                s_dict['zero-one-loss'].append(np.mean(scores_m1[count]))\n",
    "                count += 1  \n",
    "s_dict_m1 = s_dict\n",
    "df = DataFrame(s_dict_m1)\n",
    "\n",
    "print('MedianSplitTree-Breiman')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test: Median split tree + RP-dense projection\n",
    "target_dim = [1, 5, 10]\n",
    "stop_rule = ['naive', 'cell_size']\n",
    "perturb = [True, False]\n",
    "scores_m2 = list()\n",
    "params_m2 = list()\n",
    "## best params: 5, naive, True\n",
    "\n",
    "##\n",
    "for t_dim in target_dim:\n",
    "    for s_rule in stop_rule:\n",
    "        for p in perturb:\n",
    "            proj_design={'name':'projmat','params':{'name':'dasgupta', 'target_dim':t_dim}}\n",
    "            if p:\n",
    "                ddiam,_,_ = data_diameter(data_tr)\n",
    "                split_design={'name':'median_perturb', 'params':{'diameter':ddiam, 'root_height':0}}\n",
    "            else:\n",
    "                split_design = {'name':'median'}\n",
    "            stop_design={'name':s_rule}\n",
    "            if s_rule == 'cell_size':\n",
    "                ddiam, _, _ = data_diameter(data_tr)\n",
    "                stop_design['params'] = {'diameter':ddiam, 'max_level':2*np.log(data_tr.shape[0])}\n",
    "            kwargs = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "            scores_m2.append(cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, flex_binary_trees, \n",
    "                                              need_ind=True, **kwargs))\n",
    "            params_m2.append([t_dim, s_rule, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_dim = [1, 5, 10]\n",
    "stop_rule = ['naive', 'cell_size']\n",
    "perturb = [True, False]\n",
    "##\n",
    "s_dict = {'target_dim':list(), 'stop_rule': list(), 'perturb':list(), 'zero-one-loss':list()}\n",
    "count = 0\n",
    "\n",
    "for t_dim in target_dim:\n",
    "    for s_rule in stop_rule:\n",
    "        for p in perturb:\n",
    "            s_dict['target_dim'].append(t_dim)\n",
    "            s_dict['stop_rule'].append(s_rule)\n",
    "            s_dict['perturb'].append(str(p))\n",
    "            s_dict['zero-one-loss'].append(np.mean(scores_m2[count]))\n",
    "            count += 1  \n",
    "s_dict_m2 = s_dict\n",
    "df = DataFrame(s_dict_m2)\n",
    "\n",
    "print('MedianSplitTree-RPdense')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test: Median split tree + RP-spare projection\n",
    "target_dim = [1, 5, 10]\n",
    "stop_rule = ['naive', 'cell_size']\n",
    "perturb = [True, False]\n",
    "scores_m3 = list()\n",
    "params_m3 = list()\n",
    "## best params: 5, naive, False\n",
    "\n",
    "##\n",
    "for t_dim in target_dim:\n",
    "    for s_rule in stop_rule:\n",
    "        for p in perturb:\n",
    "            proj_design={'name':'projmat','params':{'name':'tomita', 'target_dim':t_dim}}\n",
    "            if p:\n",
    "                ddiam,_,_ = data_diameter(data_tr)\n",
    "                split_design={'name':'median_perturb', 'params':{'diameter':ddiam, 'root_height':0}}\n",
    "            else:\n",
    "                split_design = {'name':'median'}\n",
    "            stop_design={'name':s_rule}\n",
    "            if s_rule == 'cell_size':\n",
    "                ddiam, _, _ = data_diameter(data_tr)\n",
    "                stop_design['params'] = {'diameter':ddiam, 'max_level':2*np.log(data_tr.shape[0])}\n",
    "            kwargs = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "            scores_m3.append(cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, flex_binary_trees, \n",
    "                                              need_ind=True, **kwargs))\n",
    "            params_m3.append([t_dim, s_rule, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_dim = [1, 5, 10]\n",
    "stop_rule = ['naive', 'cell_size']\n",
    "perturb = [True, False]\n",
    "##\n",
    "s_dict = {'target_dim':list(), 'stop_rule': list(), 'perturb':list(), 'zero-one-loss':list()}\n",
    "count = 0\n",
    "\n",
    "for t_dim in target_dim:\n",
    "    for s_rule in stop_rule:\n",
    "        for p in perturb:\n",
    "            s_dict['target_dim'].append(t_dim)\n",
    "            s_dict['stop_rule'].append(s_rule)\n",
    "            s_dict['perturb'].append(str(p))\n",
    "            s_dict['zero-one-loss'].append(np.mean(scores_m3[count]))\n",
    "            count += 1  \n",
    "s_dict_m3 = s_dict\n",
    "df = DataFrame(s_dict_m3)\n",
    "\n",
    "print('5 fold CV-loss of MedianSplitTree-RPsparse')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######-------- Testing adaptive tree construction rules with fixed params (chosen from best above)\n",
    "\n",
    "\n",
    "### Test adaptive RP-tree (Kpotufe's paper)\n",
    "scores_k = cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, master_trees)\n",
    "#ddiam,_,_ = data_diameter(data_tt)\n",
    "#print(\"diameter of the entire data is %f\" %ddiam)\n",
    "#scores_k = cross_valid_eval(data_tt, labels_tt, 2, zero_one_loss, master_trees)\n",
    "print('5 fold CV-loss of adaptive RP-tree in Kpotufe is %f' %np.mean(scores_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_mtree = master_trees(data_tr, labels_tr)\n",
    "#test_mtree.build_master_trees()\n",
    "#test_mtree.leaves_list\n",
    "#c = print_mtree_leaves(test_mtree)\n",
    "#c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Test adaptive CART-decision tree + Breiman's features selection/projection rule\n",
    "proj_design={'name':'projmat','params':{'name':'breiman','sparsity':3,'target_dim':10}}\n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "split_design={'name':'cart'} \n",
    "stop_design={'name': 'cell_size', 'params':{'diameter':ddiam}}\n",
    "slave_tree_params = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "kwargs = {'child_slave_tree_params':slave_tree_params, 'max_height':2*np.log(data_tr.shape[0])}\n",
    "kwargs['max_height']=12\n",
    "\n",
    "####### \n",
    "scores_k1 = cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, master_trees, **kwargs)\n",
    "#scores_k1 = cross_valid_eval(data_tt, labels_tt, 5, zero_one_loss, master_trees, slave_tree=base_slave_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('5 fold CV-Loss of adaptive CART-Breiman is %f' %np.mean(scores_k1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Test adaptive CART-decision tree + RP-dense projection rule\n",
    "proj_design={'name':'projmat','params':{'name':'dasgupta','target_dim':10}}\n",
    "split_design={'name':'cart'} \n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "stop_design={'name': 'cell_size', 'params':{'diameter':ddiam}}\n",
    "slave_tree_params = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "kwargs = {'child_slave_tree_params':slave_tree_params, 'max_height':2*np.log(data_tr.shape[0])}\n",
    "\n",
    "scores_k2 = cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, master_trees, **kwargs)\n",
    "#scores_k2 = cross_valid_eval(data_tt, labels_tt, 5, zero_one_loss, master_trees, slave_tree=base_slave_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('5 fold CV-Loss of adaptive CART-RPdense is %f' %np.mean(scores_k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Test adaptive CART-decision tree + RP-sparse projection rule\n",
    "proj_design={'name':'projmat','params':{'name':'tomita','target_dim':1}}\n",
    "split_design={'name':'cart'} \n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "stop_design={'name': 'cell_size', 'params':{'diameter':ddiam}}\n",
    "slave_tree_params = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "kwargs = {'child_slave_tree_params':slave_tree_params, 'max_height':2*np.log(data_tr.shape[0])}\n",
    "##\n",
    "scores_k3 = cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, master_trees, **kwargs)\n",
    "#scores_k3 = cross_valid_eval(data_tt, labels_tt, 5, zero_one_loss, master_trees, slave_tree=base_slave_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('5 fold CV-Loss of adaptive CART-RPsparse is %f' %np.mean(scores_k3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Test adaptive median-split tree + Breiman's features selection/projection rule\n",
    "proj_design={'name':'projmat','params':{'name':'breiman','sparsity':10,'target_dim':10}}\n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "split_design={'name':'median_perturb', 'params':{'diameter':ddiam}} \n",
    "stop_design={'name': 'cell_size', 'params':{'diameter':ddiam}}\n",
    "slave_tree_params = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "kwargs = {'child_slave_tree_params':slave_tree_params, 'max_height':2*np.log(data_tr.shape[0])}\n",
    "scores_km1 = cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, master_trees, **kwargs)\n",
    "#scores_km1 = cross_valid_eval(data_tt, labels_tt, 5, zero_one_loss, master_trees, slave_tree=base_slave_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('5 fold CV-Loss of adaptive MedianSplit-Breiman is %f' %np.mean(scores_km1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Test adaptive median-split tree + RP-dense projection rule\n",
    "proj_design={'name':'projmat','params':{'name':'dasgupta','target_dim':5}}\n",
    "split_design={'name':'median'} \n",
    "stop_design={'name': 'naive'}\n",
    "slave_tree_params = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "kwargs = {'child_slave_tree_params':slave_tree_params, 'max_height':2*np.log(data_tr.shape[0])}\n",
    "scores_km2 = cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, master_trees, **kwargs)\n",
    "#scores_km2 = cross_valid_eval(data_tt, labels_tt, 5, zero_one_loss, master_trees, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('5 fold CV-Loss of adaptive MedianSplit-RPdense is %f' %np.mean(scores_km2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Test adaptive median-split tree + RP-sparse projection rule\n",
    "data_ind = np.ones(data_tr.shape[0], dtype=bool)\n",
    "proj_design={'name':'projmat','params':{'name':'tomita','target_dim':5}}\n",
    "split_design={'name':'median'} \n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "stop_design={'name': 'cell_size',\"params\":{'diameter':ddiam}}\n",
    "slave_tree_params = {'proj_design':proj_design, 'split_design':split_design, 'stop_design':stop_design}\n",
    "kwargs = {'child_slave_tree_params':slave_tree_params, 'max_height':2*np.log(data_tr.shape[0])}\n",
    "scores_km3 = cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, master_trees, **kwargs)\n",
    "#scores_km3 = cross_valid_eval(data_tt, labels_tt, 5, zero_one_loss, master_trees, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('5 fold CV-Loss of adaptive MedianSplit-RPsparse is %f' %np.mean(scores_km3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############--- Testing Forest ensembles + X tree method\n",
    "n_trees_list = [10,100,500]\n",
    "n_samples_list = [10, 50, 100, 200]\n",
    "n_features = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test: forest + CART breiman-tree   \n",
    "## NOW TESTING ------------------------------\n",
    "## Best params \n",
    "proj_design={'name':'projmat','params':{'name':'breiman','sparsity':1,'target_dim':8}}\n",
    "split_design={'name':'cart'} \n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "stop_design={'name':'cell_size', 'diameter':ddiam}\n",
    "kwargs = {'tree_type':{\"tree\":'flex','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'class'}\n",
    "scores_fc1 = list()\n",
    "params_fc1 = list()\n",
    "for n_trees in n_trees_list:\n",
    "    for n_samples in n_samples_list:\n",
    "        kwargs['n_trees'] = n_trees\n",
    "        kwargs['n_samples'] = n_samples\n",
    "        kwargs['n_features'] = n_features\n",
    "        scores_fc1.append(cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, forest, **kwargs))\n",
    "        params_fc1.append([n_trees, n_samples])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test error\n",
    "proj_design={'name':'projmat','params':{'name':'breiman','sparsity':3,'target_dim':10}}\n",
    "split_design={'name':'cart'} \n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "stop_design={'name':'cell_size', 'diameter':ddiam}\n",
    "kwargs = {'tree_type':{\"tree\":'flex','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'class'}\n",
    "scores_fc1_test = cross_valid_eval(data_tt, labels_tt, 5, zero_one_loss, forest, **kwargs)\n",
    "display(np.mean(scores_fc1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test: forest + CART + RP-dense   \n",
    "\n",
    "## best params (they are roughly the same): 10, 10\n",
    "proj_design={'name':'projmat','params':{'name':'dasgupta', 'target_dim':10}}\n",
    "split_design={'name':'cart'} \n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "stop_design={'name':'cell_size', 'diameter':ddiam}\n",
    "kwargs = {'tree_type':{\"tree\":'flex','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'class'}\n",
    "scores_fc2 = list()\n",
    "params_fc2 = list()\n",
    "for n_trees in n_trees_list:\n",
    "    for n_samples in n_samples_list:\n",
    "        kwargs['n_trees'] = n_trees\n",
    "        kwargs['n_samples'] = n_samples\n",
    "        kwargs['n_features'] = n_features\n",
    "        scores_fc2.append(cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, forest, **kwargs))\n",
    "        params_fc2.append([n_trees, n_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test error\n",
    "proj_design={'name':'projmat','params':{'name':'dasgupta', 'target_dim':10}}\n",
    "split_design={'name':'cart'} \n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "stop_design={'name':'cell_size', 'diameter':ddiam}\n",
    "kwargs = {'tree_type':{\"tree\":'flex','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'class'}\n",
    "scores_fc2_test = cross_valid_eval(data_tt, labels_tt, 5, zero_one_loss, forest, **kwargs)\n",
    "display(np.mean(scores_fc2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test: forest + CART + RP-sparse   \n",
    "proj_design={'name':'projmat','params':{'name':'tomita', 'target_dim':1}}\n",
    "split_design={'name':'cart'} \n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "stop_design={'name':'cell_size', 'diameter':ddiam}\n",
    "kwargs = {'tree_type':{\"tree\":'flex','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'class'}\n",
    "scores_fc3 = list()\n",
    "params_fc3 = list()\n",
    "for n_trees in n_trees_list:\n",
    "    for n_samples in n_samples_list:\n",
    "        kwargs['n_trees'] = n_trees\n",
    "        kwargs['n_samples'] = n_samples\n",
    "        kwargs['n_features'] = n_features\n",
    "        scores_fc3.append(cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, forest, **kwargs))\n",
    "        params_fc3.append([n_trees, n_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test: Forest + adaptive CART Breiman\n",
    "\n",
    "#### NEED TO FIRST MODIFY FOREST class to allow parameter passing for master trees\n",
    "proj_design={'name':'projmat','params':{'name':'breiman','sparsity':3,'target_dim':10}}\n",
    "split_design={'name':'cart'} \n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "stop_design={'name':'cell_size', 'diameter':ddiam}\n",
    "kwargs = {'tree_type':{\"tree\":'master','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'class'}\n",
    "scores_fc_k1 = list()\n",
    "params_fc_k1 = list()\n",
    "\n",
    "kwargs['n_trees'] = 10\n",
    "kwargs['n_samples'] = 100\n",
    "\n",
    "scores_fc_k1.append(cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, forest, **kwargs))\n",
    "params_fc_k1.append([n_trees, n_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proj_design={'name':'projmat','params':{'name':'breiman','sparsity':3,'target_dim':10}}\n",
    "split_design={'name':'cart'} \n",
    "ddiam, _, _ = data_diameter(data_tr)\n",
    "stop_design={'name':'cell_size', 'diameter':ddiam}\n",
    "kwargs = {'tree_type':{\"tree\":'master','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'class'}\n",
    "\n",
    "scores_fc_k1 = cross_valid_eval(data_tt, labels_tt, 5, zero_one_loss, forest, **kwargs)\n",
    "display(np.mean(scores_fc_k1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test: forest+ adaptiveRP\n",
    "kwargs = {'tree_type':{\"tree\":'master'},'predictor_type':'class','n_trees':100, 'n_samples':50, 'n_features':5}\n",
    "\n",
    "scores = cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, forest, **kwargs)\n",
    "score4 = np.mean(scores)\n",
    "score4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test: forest + naive RP\n",
    "proj_design={'name':'projmat','params':{'name':'dasgupta'}}\n",
    "split_design={'name':'median_perturb', 'params':{'regress':False}}\n",
    "stop_design={'name':'naive'}\n",
    "kwargs = {'tree_type':{\"tree\":'flex','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'class','n_trees':100, 'n_samples':50, 'n_features':5}\n",
    "\n",
    "scores = cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, forest, **kwargs)\n",
    "score5 = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test: forest + naive RP\n",
    "proj_design={'name':'projmat','params':{'name':'dasgupta'}}\n",
    "split_design={'name':'median_perturb', 'params':{'regress':False}}\n",
    "stop_design={'name':'naive'}\n",
    "kwargs = {'tree_type':{\"tree\":'flex','proj_design':proj_design,'split_design':split_design,'stop_design':stop_design}, \n",
    "          'predictor_type':'class','n_trees':100, 'n_samples':50, 'n_features':5}\n",
    "\n",
    "scores = cross_valid_eval(data_tr, labels_tr, 5, zero_one_loss, forest, **kwargs)\n",
    "score5 = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_scores = np.mean(np.array(scores_fc_k1), axis=1)\n",
    "for res in zip(params_fc_k1, avg_scores):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My observations from preliminary experiments:\n",
    "- Forest type estimators seem to perform uniformly worse than single-tree based estimators on sonar data\n",
    "- Trees with adaptive pruning strategy (inspired by Kpotufe's paper) seems to perform significantly better than others, though it takes a lot more time \n",
    "- But combining forest with adaptive pruning doesn't help; it achieves similar bad performance as forest with naive stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
